{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Zt0JEl38SWX"
   },
   "source": [
    "# Keras 神经网络剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwBSTsryazfT"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8qFbkru8FKu"
   },
   "source": [
    "## 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pn836LSTNSHA"
   },
   "outputs": [],
   "source": [
    "! pip uninstall -y tensorflow\n",
    "! pip uninstall -y tf-nightly\n",
    "! pip install -U tf-nightly-gpu\n",
    "\n",
    "! pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ykjgo4UNXmD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mydXeQlDNbnR"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBYltugp-MdR"
   },
   "source": [
    "## 准备训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_MJqxz5z2dh"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "  input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "  input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmdnYKPK--5L"
   },
   "source": [
    "## 使用未剪枝模型训练MNIST 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1p_y3gPWeW2"
   },
   "source": [
    "### Build the MNIST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLg0SGdYp2Q6"
   },
   "outputs": [],
   "source": [
    "l = tf.keras.layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    l.Conv2D(\n",
    "        32, 5, padding='same', activation='relu', input_shape=input_shape),\n",
    "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    l.BatchNormalization(),\n",
    "    l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    l.Flatten(),\n",
    "    l.Dense(1024, activation='relu'),\n",
    "    l.Dropout(0.4),\n",
    "    l.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dHynoso_xXF"
   },
   "source": [
    "### Train the model to reach an accuracy >99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGo0UICmA8J3"
   },
   "source": [
    "\n",
    "Load [TensorBoard](https://www.tensorflow.org/tensorboard) to monitor the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJyS_IcQBBqg"
   },
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "print('Writing training logs to ' + logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfgjuOLj_mFu"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7c-fsQpTzOr"
   },
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch=0)]\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLgMavDVCde5"
   },
   "source": [
    "### Save the original model for size comparison later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8E6U7GUIx5r9"
   },
   "outputs": [],
   "source": [
    "# Backend agnostic way to save/restore models\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "print('Saving model to: ', keras_file)\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWiQbobKC0NZ"
   },
   "source": [
    "## 训练剪枝模型\n",
    "\n",
    "我们提供了一个prune_low_magnitude（）API来训练已删除连接的模型。\n",
    "\n",
    "例如，典型配置将通过从步骤2,000开始每100个步骤（也称为epochs）修剪连接来瞄准75％的稀疏度。 有关可能配置的更多详细信息，请参阅github文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfJ6Tm3KXMFY"
   },
   "source": [
    "### 逐层构建剪枝模型\n",
    "在此示例中，我们将展示如何在层级使用API，并构建修剪的MNIST模型。\n",
    "\n",
    "在这种情况下，`prune_low_magnitude（`）\n",
    "接收我们想要修剪其重量的Keras层作为参数。\n",
    "\n",
    "此功能需要修剪参数，在训练期间配置修剪算法。 有关详细文档，请参阅我们的github页面。 这里使用的参数表示：\n",
    "\n",
    "\n",
    "1.   **Sparsity.** PolynomialDecay is used across the whole training process. We start at the sparsity level 50% and gradually train the model to reach 90% sparsity. X% sparsity means that X% of the weight tensor is going to be pruned away.\n",
    "2.   **Schedule**. Connections are pruned starting from step 2000 to the end of training, and runs every 100 steps. The reasoning behind this is that we want to train the model without pruning for a few epochs to reach a certain accuracy, to aid convergence. Furthermore, we give the model some time to recover after each pruning step, so pruning does not happen on every step. We set the pruning frequency to 100.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ip8qCtSlU3TQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity import keras as sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tA9rnRUrebTE"
   },
   "source": [
    "为了演示如何保存和恢复修剪的keras模型，在下面的示例中，我们首先训练模型10个epochs，将其保存到磁盘，最后恢复并继续训练2个epochs。 逐渐稀疏，四个重要参数是begin_sparsity，final_sparsity，begin_step和end_step。 前三个是直截了当的。 让我们根据训练样本数量，批量大小和训练的总时期来计算结束步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rrs-xoB2cSSQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 12\n",
    "num_train_samples = x_train.shape[0]\n",
    "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
    "print('End step: ' + str(end_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Shz-r2RiqFca"
   },
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                   final_sparsity=0.90,\n",
    "                                                   begin_step=2000,\n",
    "                                                   end_step=end_step,\n",
    "                                                   frequency=100)\n",
    "}\n",
    "\n",
    "pruned_model = tf.keras.Sequential([\n",
    "    sparsity.prune_low_magnitude(\n",
    "        l.Conv2D(32, 5, padding='same', activation='relu'),\n",
    "        input_shape=input_shape,\n",
    "        **pruning_params),\n",
    "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    l.BatchNormalization(),\n",
    "    sparsity.prune_low_magnitude(\n",
    "        l.Conv2D(64, 5, padding='same', activation='relu'), **pruning_params),\n",
    "    l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    l.Flatten(),\n",
    "    sparsity.prune_low_magnitude(l.Dense(1024, activation='relu'),\n",
    "                                 **pruning_params),\n",
    "    l.Dropout(0.4),\n",
    "    sparsity.prune_low_magnitude(l.Dense(num_classes, activation='softmax'),\n",
    "                                 **pruning_params)\n",
    "])\n",
    "\n",
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YczppQ6vEPJg"
   },
   "source": [
    "Load Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIO8UpZyEYkp"
   },
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "print('Writing training logs to ' + logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKb8XpDkA8TN"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2166laKE_N6"
   },
   "source": [
    "### 训练模型\n",
    "\n",
    "Start pruning from step 2000 when accuracy >98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGoOTwQRzEP4"
   },
   "outputs": [],
   "source": [
    "pruned_model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
    "# step. Also add a callback to add pruning summaries to tensorboard\n",
    "callbacks = [\n",
    "    sparsity.UpdatePruningStep(),\n",
    "    sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
    "]\n",
    "\n",
    "pruned_model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = pruned_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Nzrm5pN1viP"
   },
   "source": [
    "### Save and restore the pruned model\n",
    "\n",
    "Continue training for two epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rd8G7srV2dcI"
   },
   "outputs": [],
   "source": [
    "_, checkpoint_file = tempfile.mkstemp('.h5')\n",
    "print('Saving pruned model to: ', checkpoint_file)\n",
    "# saved_model() sets include_optimizer to True by default. Spelling it out here\n",
    "# to highlight.\n",
    "tf.keras.models.save_model(pruned_model, checkpoint_file, include_optimizer=True)\n",
    "\n",
    "with sparsity.prune_scope():\n",
    "  restored_model = tf.keras.models.load_model(checkpoint_file)\n",
    "\n",
    "restored_model.fit(x_train, y_train,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=2,\n",
    "                   verbose=1,\n",
    "                   callbacks=callbacks,\n",
    "                   validation_data=(x_test, y_test))\n",
    "\n",
    "score = restored_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1vV7pZrW5TQh"
   },
   "source": [
    "在上面的示例中，需要注意的一些事项是：\n",
    "\n",
    "\n",
    "*   保存模型时，必须将include_optimizer设置为True。 我们需要在训练期间保留优化器的状态，以便修剪工作正常。\n",
    "*   加载修剪后的模型时，需要使用prune_scope()进行反序列化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMTFhyc0vAA3"
   },
   "source": [
    "### 在发布模型之前，从修剪过的模型中去除pruning wrappers \n",
    "在导出服务模型之前，您需要调用`strip_pruning` API来从模型中剥离修剪包装器，因为它只需要进行培训。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyCjjpUjvImz"
   },
   "outputs": [],
   "source": [
    "final_model = sparsity.strip_pruning(pruned_model)\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B63tViKp_qLK"
   },
   "outputs": [],
   "source": [
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "print('Saving pruned model to: ', pruned_keras_file)\n",
    "\n",
    "# No need to save the optimizer with the graph for serving.\n",
    "tf.keras.models.save_model(final_model, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhXHuBAGOBvY"
   },
   "source": [
    "### 比较压缩后未修剪模型与修剪模型的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hk4DoZTIy2uU"
   },
   "outputs": [],
   "source": [
    "_, zip1 = tempfile.mkstemp('.zip') \n",
    "with zipfile.ZipFile(zip1, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(keras_file)\n",
    "print(\"Size of the unpruned model before compression: %.2f Mb\" % \n",
    "      (os.path.getsize(keras_file) / float(2**20)))\n",
    "print(\"Size of the unpruned model after compression: %.2f Mb\" % \n",
    "      (os.path.getsize(zip1) / float(2**20)))\n",
    "\n",
    "_, zip2 = tempfile.mkstemp('.zip') \n",
    "with zipfile.ZipFile(zip2, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(pruned_keras_file)\n",
    "print(\"Size of the pruned model before compression: %.2f Mb\" % \n",
    "      (os.path.getsize(pruned_keras_file) / float(2**20)))\n",
    "print(\"Size of the pruned model after compression: %.2f Mb\" % \n",
    "      (os.path.getsize(zip2) / float(2**20)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dayb_w_GqWs_"
   },
   "source": [
    "### 修剪整个模型\n",
    "\n",
    "`prune_low_magnitude`函数也可以应用于整个Keras模型。\n",
    "\n",
    "在这种情况下，该算法将应用于所有可用于权重修剪的层（API知道）。 API知道的不能用于权重修剪的图层将被忽略，API的未知图层将导致错误。\n",
    "\n",
    "*如果您的模型具有API不知道如何修剪其权重的层，但是完全可以保留“未修剪”，那么只需在每层中应用API。*\n",
    "\n",
    "关于修剪配置，相同的设置适用于模型中的所有可修剪层。\n",
    "\n",
    "另外值得注意的是，修剪不会保留与原始模型关联的优化程序。 因此，有必要使用新的优化器重新编译已修剪的模型。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-W7Sj8fZjeb"
   },
   "source": [
    "在我们继续这个例子之前，让我们讨论一下您可能已经拥有序列化预训练Keras模型的常见用例，您希望对其进行权重修剪。 我们将采用之前训练过的原始MNIST模型来展示其工作原理。 在这种情况下，您首先将模型加载到内存中，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJm1SJxfqy2e"
   },
   "outputs": [],
   "source": [
    "# Load the serialized model\n",
    "loaded_model = tf.keras.models.load_model(keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etYrWnrSMpB5"
   },
   "source": [
    "然后你可以修剪加载的模型并编译修剪后的模型进行训练。 在这种情况下，训练将从步骤0重新开始。鉴于我们加载的模型已达到令人满意的准确度，我们可以立即开始修剪。 结果，我们在这里将begin_step设置为0，并且仅训练另外四个epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CabnOldzrSN2"
   },
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
    "print(end_step)\n",
    "\n",
    "new_pruning_params = {\n",
    "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                   final_sparsity=0.90,\n",
    "                                                   begin_step=0,\n",
    "                                                   end_step=end_step,\n",
    "                                                   frequency=100)\n",
    "}\n",
    "\n",
    "new_pruned_model = sparsity.prune_low_magnitude(model, **new_pruning_params)\n",
    "new_pruned_model.summary()\n",
    "\n",
    "new_pruned_model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qCipfCnaY7g"
   },
   "source": [
    "Load tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPBlcTXWB9tx"
   },
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "print('Writing training logs to ' + logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X43ix4NZCDNS"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2hLPO7KNKq_"
   },
   "source": [
    "### 将模型训练另外四个epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36hymxokrnbw"
   },
   "outputs": [],
   "source": [
    "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
    "# step. Also add a callback to add pruning summaries to tensorboard\n",
    "callbacks = [\n",
    "    sparsity.UpdatePruningStep(),\n",
    "    sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
    "]\n",
    "\n",
    "new_pruned_model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = new_pruned_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0yphbuRarfT"
   },
   "source": [
    "### 导出已修剪的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anFHUpCXrxMe"
   },
   "outputs": [],
   "source": [
    "final_model = sparsity.strip_pruning(pruned_model)\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CmEtxHEso7g"
   },
   "outputs": [],
   "source": [
    "_, new_pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "print('Saving pruned model to: ', new_pruned_keras_file)\n",
    "tf.keras.models.save_model(final_model, new_pruned_keras_file, \n",
    "                        include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YT8YqXAza3Tt"
   },
   "source": [
    "压缩后的模型尺寸与逐层修剪的模型尺寸相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtKANC0hs2RR"
   },
   "outputs": [],
   "source": [
    "_, zip3 = tempfile.mkstemp('.zip')\n",
    "with zipfile.ZipFile(zip3, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(new_pruned_keras_file)\n",
    "print(\"Size of the pruned model before compression: %.2f Mb\" \n",
    "      % (os.path.getsize(new_pruned_keras_file) / float(2**20)))\n",
    "print(\"Size of the pruned model after compression: %.2f Mb\" \n",
    "      % (os.path.getsize(zip3) / float(2**20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXrLGUPfIwvV"
   },
   "source": [
    "## 转换为TensorFlow Lite\n",
    "\n",
    "最后，您可以将已修剪的模型转换为可在目标后端运行的格式。 Tensorflow Lite是可用于部署到移动设备的示例格式。 要转换为Tensorflow Lite图形，您需要使用TFLiteConverter，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1f9Eb2K0bcJG"
   },
   "source": [
    "### 使用TFLiteConverter转换模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ctqfiix-H-x7"
   },
   "outputs": [],
   "source": [
    "tflite_model_file = '/tmp/sparse_mnist.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_model_file, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdIiNnrPANcw"
   },
   "source": [
    "压缩后TensorFlow Lite模型的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYMSXAU1AUYI"
   },
   "outputs": [],
   "source": [
    "_, zip_tflite = tempfile.mkstemp('.zip')\n",
    "with zipfile.ZipFile(zip_tflite, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(tflite_model_file)\n",
    "print(\"Size of the tflite model before compression: %.2f Mb\" \n",
    "      % (os.path.getsize(tflite_model_file) / float(2**20)))\n",
    "print(\"Size of the tflite model after compression: %.2f Mb\" \n",
    "      % (os.path.getsize(zip_tflite) / float(2**20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBqUX1qopV1k"
   },
   "source": [
    "### 评估TensorFlow Lite模型的准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5AY-TuivmbP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "def eval_model(interpreter, x_test, y_test):\n",
    "  total_seen = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for img, label in zip(x_test, y_test):\n",
    "    inp = img.reshape((1, 28, 28, 1))\n",
    "    total_seen += 1\n",
    "    interpreter.set_tensor(input_index, inp)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_index)\n",
    "    if np.argmax(predictions) == np.argmax(label):\n",
    "      num_correct += 1\n",
    "\n",
    "    if total_seen % 1000 == 0:\n",
    "        print(\"Accuracy after %i images: %f\" %\n",
    "              (total_seen, float(num_correct) / float(total_seen)))\n",
    "\n",
    "  return float(num_correct) / float(total_seen)\n",
    "\n",
    "print(eval_model(interpreter, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zalu5Ng7D7xi"
   },
   "source": [
    "### 量化TensorFlow Lite模型\n",
    "\n",
    "您可以将修剪与其他优化技术（如训练后量化）结合起来。 作为回顾，训练后量化将权重转换为8位精度，作为从keras模型到TFLite平坦缓冲区的模型转换的一部分，从而使模型大小减少4倍。\n",
    "\n",
    "在下面的示例中，我们采用修剪的keras模型，使用训练后量化转换它，检查尺寸减小并验证其准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbTNqf1KER0z"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "tflite_quant_model_file = '/tmp/sparse_mnist_quant.tflite'\n",
    "with open(tflite_quant_model_file, 'wb') as f:\n",
    "  f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6a8bH_-EXeR"
   },
   "outputs": [],
   "source": [
    "_, zip_tflite = tempfile.mkstemp('.zip')\n",
    "with zipfile.ZipFile(zip_tflite, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "  f.write(tflite_quant_model_file)\n",
    "print(\"Size of the tflite model before compression: %.2f Mb\" \n",
    "      % (os.path.getsize(tflite_quant_model_file) / float(2**20)))\n",
    "print(\"Size of the tflite model after compression: %.2f Mb\" \n",
    "      % (os.path.getsize(zip_tflite) / float(2**20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMuuxZs_QxMt"
   },
   "source": [
    "量化模型的大小约为原始模型的1/4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dkv9mauCEami"
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_quant_model_file))\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "print(eval_model(interpreter, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygzK3KZcoZ-w"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we showed you how to create *sparse models* with the TensorFlow model optimization toolkit weight pruning API. Right now, this allows you to create models that take significant less space on disk. The resulting model can also be more efficiently implemented to avoid computation; in the future TensorFlow Lite will provide such capabilities.\n",
    "\n",
    "More specifically, we walked you through an end-to-end example of training a simple MNIST model that used the weight pruning API. We showed you how to convert it to the Tensorflow Lite format for mobile deployment, and demonstrated how with simple file compression the model size was reduced 5x.\n",
    "\n",
    "We encourage you to try this new capability on your Keras models, which can be particularly important for deployment in resource-constraint environments. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "JCkPHee-alKM"
   ],
   "name": "pruning_with_keras.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
