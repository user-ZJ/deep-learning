


pytorch:
https://github.com/Eric-mingjie/rethinking-network-pruning
https://github.com/jacobgil/pytorch-pruning
https://github.com/synxlin/nn-compression
https://github.com/eeric/channel_prune
https://github.com/he-y/soft-filter-pruning


keras:
https://github.com/saadmanrafat/pruning-cnn-using-rl
https://github.com/tensorflow/model-optimization
https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras

caffe
https://github.com/yihui-he/channel-pruning
https://github.com/zepx/pytorch-weight-prune


tensorflow:
https://github.com/Ewenwan/MVision/tree/master/CNN/Deep_Compression/pruning
https://github.com/garion9013/impl-pruning-TF  
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning


# tensorflow剪枝参数说明
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning     


	eg:
	--pruning_hparams=name=cifar10_pruning,begin_pruning_step=10000,end_pruning_step=100000,target_sparsity=0.9,
	sparsity_function_begin_step=10000,sparsity_function_end_step=100000

	end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs
	--pruning_hparams=name=cifar10_pruning,begin_pruning_step=2000,end_pruning_step=end_step,initial_sparsity=0.5,target_sparsity=0.9,pruning_frequency=100

    name: string
      修剪规范的名称。 用于在公共tensorflow name_scope下添加摘要和操作
    begin_pruning_step: integer
      开始修剪的global step，默认为0
    end_pruning_step: integer
      终止修剪的global step。 默认为-1表示修剪持续到训练停止
    weight_sparsity_map: list of strings
       逗号分隔的权重变量名称列表：目标稀疏度对。 对于不在此列表中的图层/权重，使用由target_sparsity超参数指定的稀疏性。默认为空
       Eg. [conv1:0.9,conv2/kernel:0.8]
    threshold_decay: float
      the decay factor to use for exponential decay of the thresholds，默认为0.0
    pruning_frequency: integer
      参数masks更新频率，每pruning_frequency步更新一次，默认值为pruning_frequency=10
    nbins: integer
      用于直方图计算，默认为nbins=256
    block_height: integer
      块中的行数，默认为1
    block_width: integer
      块中的列数，默认为1
    block_pooling_function: string
      是否在块中执行平均（AVG）或最大（MAX）池化，默认为平均池化（AVG）
    initial_sparsity: float
      初始稀疏度值，默认值为0.0
    target_sparsity: float
      目标稀疏度值，默认值为0.5
    sparsity_function_begin_step: integer
	  渐进稀疏功能开始生效的global step，默认值为0
    sparsity_function_end_step: integer
      渐进稀疏功能终止的global step，默认值为100
    sparsity_function_exponent: float
      exponent = 1 表示initial_sparsity和target_sparsity之间的稀疏度是线性变换.
      exponent > 1 表示后面的变换比开始时慢
	  默认值为3
    use_tpu: False
      Indicates whether to use TPU

    We use the following sparsity function:

    num_steps = (sparsity_function_end_step -
                 sparsity_function_begin_step)/pruning_frequency
    sparsity(step) = (initial_sparsity - target_sparsity)*
                     [1-step/(num_steps -1)]**exponent + target_sparsity


# 从训练的图表中删除修剪操作
训练模型后，必须删除上述步骤中添加到图形中的辅助变量（掩模，阈值）和修剪操作。这可以使用该strip_pruning_vars实用程序来完成。

strip_pruning_vars生成二进制GraphDef，其中变量已转换为常量。特别地，从图中移除阈值变量，并且将掩码变量与对应的权重张量融合以产生masked_weight张量。该张量是稀疏的，具有与权重张量相同的大小，并且稀疏性由上面的target_sparsity或者weight_sparsity_map超参数设置


$ bazel build -c opt contrib/model_pruning:strip_pruning_vars
$ bazel-bin/contrib/model_pruning/strip_pruning_vars --checkpoint_dir=/path/to/checkpoints/ --output_node_names=graph_node1,graph_node2 --output_dir=/tmp --filename=pruning_stripped.pb
