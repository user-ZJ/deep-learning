TensorFlow1æ˜¯ç¬¦å·å¼ç¼–ç¨‹ï¼Œå…ˆåˆ›å»ºè®¡ç®—å›¾åè¿è¡Œçš„ç¼–ç¨‹æ–¹å¼

tensorflow2 æ”¯æŒåŠ¨æ€å›¾ä¼˜å…ˆæ¨¡å¼ï¼Œåœ¨è®¡ç®—æ—¶å¯ä»¥åŒæ—¶è·å¾—è®¡ç®—å›¾ä¸æ•°å€¼ç»“æœï¼Œå¯ä»¥åœ¨ä»£ç ä¸­è°ƒè¯•å¹¶å®æ—¶æ‰“å°æ•°æ®ï¼Œæ­å»ºç½‘ç»œä¹Ÿåƒæ­ç§¯æœ¨ä¸€æ ·å±‚å±‚å †å ï¼Œç¬¦åˆè½¯ä»¶å¼€å‘æ€ç»´

## 1 tensorflow2å®‰è£…åŠä½¿ç”¨

```shell
# ä½¿ç”¨å›½å†…æ¸…åæºå®‰è£…TensorFlow CPU ç‰ˆæœ¬
pip install -U tensorflow -i https://pypi.tuna.tsinghua.edu.cn/simple
# ä½¿ç”¨æ¸…åæºå®‰è£…TensorFlow GPU ç‰ˆæœ¬,"-U"å‚æ•°æŒ‡å®šå¦‚æœå·²å®‰è£…æ­¤åŒ…ï¼Œåˆ™æ‰§è¡Œå‡çº§å‘½ä»¤
pip install -U tensorflow-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple
# æµ‹è¯•
import tensorflow as tf
tf.test.is_gpu_available()
tf.__version__

# åˆ›å»ºåä¸ºtf2 çš„è™šæ‹Ÿç¯å¢ƒï¼Œå¹¶æ ¹æ®é¢„è®¾ç¯å¢ƒåtensorflow-gpu
# è‡ªåŠ¨å®‰è£…CUDA,cuDNN,TensorFlow GPU ç­‰
conda create -n tf2 tensorflow-gpu
# æ¿€æ´»tf2 è™šæ‹Ÿç¯å¢ƒ
conda activate tf2
```

```python
# è®¾ç½®GPUæ˜¾å­˜ä½¿ç”¨æ–¹å¼
# è·å–GPUåˆ—è¡¨
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    # è®¾ç½®GPUä¸ºå¢é•¿å¼å ç”¨
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True) 
  except RuntimeError as e:
    # æ‰“å°å¼‚å¸¸
    print(e)
```

```py
with tf.device('/cpu:0'):
	cpu_a=tf.random.normal([1,n])
	cpu_b=tf.random.normal([n,1])
	print(cpu_a.device,cpu_b,device)
	c=tf.matmul(cpu_a,cpu_b)
with tf.device('/gpu:0'):
	gpu_a=tf.random.normal([1,n])
	gpu_b=tf.random.normal([n,1])
	print(gpu_a.device,gpu_b.device)
	c=matmul(gpu_a,gpu_b)
```

```py
#è‡ªåŠ¨æ¢¯åº¦
import tensorflow as tf
a = tf.constant(1.)
b = tf.constant(2.)
c = tf.constant(3.)
w = tf.constant(4.)
with tf.GradientTape as tape: #æ„å»ºæ¢¯åº¦ç¯å¢ƒ
	tape.watch([w]) #å°†wåŠ å…¥æ¢¯åº¦è·Ÿè¸ªåˆ—è¡¨
	#æ„å»ºè®¡ç®—è¿‡ç¨‹ï¼Œå‡½æ•°è¡¨è¾¾å¼
	y = a * w**2 + b * w + c
#è‡ªåŠ¨æ±‚å¯¼
[dy_dw] = tape.gradient(y,[w])
print(dy_dw)
```

## 2 å®ä¾‹

### 2.1 ç®€å•æ‰‹å†™æ•°å­—å®ä¾‹

```python
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,optimizers,datasets

# è®¾ç½®GPUæ˜¾å­˜ä½¿ç”¨æ–¹å¼
# è·å–GPUåˆ—è¡¨
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    # è®¾ç½®GPUä¸ºå¢é•¿å¼å ç”¨
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True) 
  except RuntimeError as e:
    # æ‰“å°å¼‚å¸¸
    print(e)

(x,y),(x_val,y_val)=datasets.mnist.load_data() #åŠ è½½mnistæ•°æ®é›†
x = 2*tf.convert_to_tensor(x,dtype=tf.float32)/255.-1 #æ ‡å‡†åŒ–æ•°æ®ï¼Œç¼©æ”¾åˆ°[-1,1]
y = tf.convert_to_tensor(y,dtype=tf.int32) 
y = tf.one_hot(y,depth=10)
print(x.shape,y.shape)
train_dataset = tf.data.from_tensor_slices((x,y)) #æ„å»ºæ•°æ®é›†å¯¹è±¡
train_dataset = train_dataset.batch(32).repeat(30) #æ‰¹é‡è®­ç»ƒ,batch_size=32,epoch=30
#æ„å»ºæ¨¡å‹
model = keras.Sequential(
	layers.Desen(256,activation='relu'),
	layers.Desen(128,activation='relu'),
	layers.Desen(10))
model.build(input_shape=(4, 28*28))
model.summary()

optimizer = optimizers.SGD(lr=0.01) #åˆ›å»ºä¼˜åŒ–å™¨
acc_meter = metrics.Accuracy()  # åˆ›å»ºå‡†ç¡®ç‡æµ‹é‡å™¨

for step, (x,y) in enumerate(db):
	with tf.GradientTape() as tape: #æ„å»ºæ¢¯åº¦è®°å½•ç¯å¢ƒ
		x = tf.reshape(x,(-1,28*28))
		out = model(x)
		y_onehot = tf.onehot(y,depth=10)
		loss = tf.square(out-y_onehot)
		loss = tf.reduce_sum(loss)/x.shape(0) 
	acc_meter.update_state(tf.argmax(out, axis=1), y)  # è®°å½•é‡‡æ ·çš„æ•°æ®
	grads = tape.gradient(loss,model.trainable_variables)  #è®¡ç®—æ¢¯åº¦
	optimizer.apply_gradient(zip(grads,model.trainable_variables)) #æ›´æ–°å‚æ•°
	if step % 200==0:
        # æ‰“å°ç»Ÿè®¡æœŸé—´çš„å¹³å‡å‡†ç¡®ç‡
        print(step, 'loss:', float(loss), 'acc:', acc_meter.result().numpy()) 
        acc_meter.reset_states()
```



## 3 æ•°æ®ç±»å‹

TensorFlowä¸­çš„åŸºæœ¬æ•°æ®ç±»å‹åŒ…å«**æ•°å€¼ç±»å‹**ï¼Œ**å­—ç¬¦ä¸²ç±»å‹**å’Œ**å¸ƒå°”ç±»å‹**

### 3.1 æ•°å€¼ç±»å‹

æ•°å€¼ç±»å‹å¯ä»¥åŒºåˆ†ä¸ºï¼š

**æ ‡é‡ï¼ˆScalarï¼‰**ï¼šå•ä¸ªçš„å®æ•°ï¼›

**å‘é‡**ï¼šnä¸ªå®æ•°çš„æœ‰åºé›†åˆï¼›

**çŸ©é˜µ**ï¼šnè¡Œmåˆ—å®æ•°çš„æœ‰åºé›†åˆã€‚

åœ¨TensorFlowä¸­ä¸ºäº†è¡¨è¾¾æ–¹ä¾¿ï¼Œä¸€èˆ¬æŠŠæ ‡é‡ã€å‘é‡ã€çŸ©é˜µç»Ÿç§°ä¸º**å¼ é‡**ï¼Œä¸åšåŒºåˆ†ï¼Œéœ€è¦æ ¹æ®å¼ é‡çš„ç»´åº¦æ•°æˆ–å½¢çŠ¶è‡ªè¡Œåˆ¤æ–­

```python
a = tf.constant(1.2)
b = tf.constant([1.2,3.3])
c = tf.constant([[1,2],[3,4]])
c = tf.constant([[[1,2],[3,4]],
				[[5,6],[7,8]]])
loss = float(a)  #é€šè¿‡float()å‡½æ•°å°†å¼ é‡è½¬æ¢ä¸ºæ™®é€šæ•°å€¼
c.numpy() #å¼ é‡è½¬æ¢ä¸ºpythonæ•°æ®
```

> å¼ é‡çš„numpy()æ–¹æ³•å¯ä»¥æ”¾å›Numpy.arrayç±»å‹çš„æ•°æ®ï¼Œæ–¹ä¾¿å¯¼å‡ºæ•°æ®åˆ°ç³»ç»Ÿå…¶å®ƒæ¨¡å—

### 3.2 å­—ç¬¦ä¸²ç±»å‹

åœ¨è¡¨ç¤ºå›¾ç‰‡æ•°æ®æ˜¯ï¼Œå¯ä»¥å…ˆè®°å½•å›¾ç‰‡çš„è·¯å¾„å­—ç¬¦ä¸²ï¼Œåœ¨é€šè¿‡é¢„å¤„ç†å‡½æ•°æ ¹æ®è·¯å¾„è¯»å–å›¾ç‰‡å¼ é‡

tf.stringæ¨¡å—ä¸­æä¾›äº†å¸¸è§çš„å­—ç¬¦ä¸²ç±»å‹çš„å·¥å…·å‡½æ•°ï¼Œå¦‚lower(),join(),length(),split()ç­‰

```py
a = tf.constant('hello world')
```

### 3.3 å¸ƒå°”ç±»å‹

ä¸ºäº†æ–¹ä¾¿è¡¨è¾¾æ¯”è¾ƒè¿ç®—æ“ä½œçš„ç»“æœï¼ŒTensorFlowæ”¯æŒå¸ƒå°”ç±»å‹çš„å¼ é‡

```py
a = tf.constant(True)
b = tf.constant([True,False]) #åˆ›å»ºå¸ƒå°”ç±»å‹å‘é‡
```

TensorFlowçš„å¸ƒå°”ç±»å‹å’ŒPythonè¯­è¨€çš„å¸ƒå°”ç±»å‹å¹¶ä¸ç­‰ä»·ï¼Œä¸èƒ½é€šç”¨

```py
a = tf.constant(True)
a is True  #ç»“æœä¸ºFalse
a == True #ä»…æ•°å€¼æ¯”è¾ƒï¼Œç»“æœä¸º<tf.Tensor: id=8,shape=(),dtype=bool,numpy=True>
```

### 3.4 ç±»å‹è½¬æ¢

tfä¸­å¸¸ç”¨çš„ç²¾åº¦ç±»å‹æœ‰tf.int16ã€tf.int32ã€tf.int64ã€tf.float16ã€tf.float32ã€tf.float64 ç­‰ï¼Œå…¶ä¸­tf.float64 å³ä¸ºtf.double

å¯¹äºå¤§éƒ¨åˆ†æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œä¸€èˆ¬ä½¿ç”¨tf.int32 å’Œtf.float32 å¯æ»¡è¶³å¤§éƒ¨åˆ†åœºåˆçš„è¿ç®—ç²¾åº¦è¦æ±‚ï¼Œéƒ¨åˆ†å¯¹ç²¾åº¦è¦æ±‚è¾ƒé«˜çš„ç®—æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ æŸäº›ç®—æ³•ï¼Œå¯ä»¥é€‰æ‹©ä½¿ç”¨tf.int64 å’Œtf.float64 ç²¾åº¦ä¿å­˜å¼ é‡ã€‚

```python
a = tf.constant(np.pi, dtype=tf.float16) # åˆ›å»ºtf.float16 ä½ç²¾åº¦å¼ é‡
a.dtype
tf.cast(a, tf.double) # è½¬æ¢ä¸ºé«˜ç²¾åº¦å¼ é‡
a = tf.constant([True, False])
tf.cast(a, tf.int32) # å¸ƒå°”ç±»å‹è½¬æ•´å‹
a = tf.constant([-1, 0, 1, 2])
tf.cast(a, tf.bool) # æ•´å‹è½¬å¸ƒå°”ç±»å‹,é0 æ•°å­—éƒ½è§†ä¸ºTrue
```

### 3.5 å¾…ä¼˜åŒ–å¼ é‡ï¼ˆVariableï¼‰

ä¸ºäº†**åŒºåˆ†éœ€è¦è®¡ç®—æ¢¯åº¦ä¿¡æ¯çš„å¼ é‡å’Œä¸éœ€è¦è®¡ç®—æ¢¯åº¦ä¿¡æ¯çš„å¼ é‡**ï¼ŒTensorFlowå¢åŠ äº†ä¸€ç§ä¸“é—¨çš„æ•°æ®ç±»å‹æ¥æ”¯æŒæ¢¯åº¦ä¿¡æ¯çš„è®°å½•ï¼štf.Variableã€‚tf.Variableç±»å‹åœ¨æ™®é€šçš„å¼ é‡ç±»å‹åŸºç¡€ä¸Šæ·»åŠ äº†nameï¼Œtrainableç­‰å±æ€§æ¥æ”¯æŒè®¡ç®—å›¾çš„æ„å»ºã€‚ç”±äºæ¢¯åº¦è¿ç®—ä¼šæ¶ˆè€—å¤§è¿çš„è®¡ç®—èµ„æºï¼Œè€Œä¸”ä¼šè‡ªåŠ¨æ›´æ–°ç›¸å…³å‚æ•°ï¼Œå¯¹äºä¸éœ€è¦ä¼˜åŒ–çš„å¼ é‡ï¼Œå¦‚ç¥ç»ç½‘ç»œçš„è¾“å…¥Xï¼Œä¸éœ€è¦é€šè¿‡tf.Variableå°è£…ï¼Œå¯¹äºéœ€è¦è®¡ç®—æ¢¯åº¦å¹¶ä¼˜åŒ–çš„å¼ é‡ï¼Œå¦‚ç¥ç»ç½‘ç»œå±‚çš„Wå’Œbï¼Œéœ€è¦é€šè¿‡tf.VariableåŒ…è£¹ä»¥ä¾¿TensorFlowè·Ÿè¸ªç›¸å…³æ¢¯åº¦ä¿¡æ¯ã€‚

```
#æ™®é€šå¼ é‡è½¬æ¢ä¸ºå¾…ä¼˜åŒ–å¼ é‡
a = tf.constant([-1,0,1,2])
aa = tf.Variable(a)
print(aa.name,aa.trainable)
# ç›´æ¥åˆ›å»ºVariable å¼ é‡
a = tf.Variable([[1,2],[3,4]]) 
```

nameå’Œtrainableå±æ€§æ˜¯Variableç‰¹æœ‰çš„å±æ€§ï¼Œnameå±æ€§ç”¨äºå‘½åè®¡ç®—å›¾ä¸­çš„å˜é‡ï¼Œè¿™å¥—å‘½åä½“ç³»æ˜¯TensorFlowå†…éƒ¨ç»´æŠ¤çš„ï¼Œä¸€èˆ¬ä¸éœ€è¦ç”¨æˆ·å…³æ³¨nameå±æ€§ï¼Œtrainableå±æ€§è¡¨å¾å½“å‰å¼ é‡æ˜¯å¦éœ€è¦è¢«ä¼˜åŒ–ï¼Œåˆ›å»ºVariable å¯¹è±¡æ—¶æ˜¯**é»˜è®¤å¯ç”¨ä¼˜åŒ–æ ‡å¿—**ï¼Œå¯ä»¥è®¾ç½®trainable=False æ¥è®¾ç½®å¼ é‡ä¸éœ€è¦ä¼˜åŒ–ã€‚  

**å¾…ä¼˜åŒ–å¼ é‡å¯è§†ä¸ºæ™®é€šå¼ é‡çš„ç‰¹æ®Šç±»å‹ï¼Œæ™®é€šå¼ é‡å…¶å®ä¹Ÿå¯ä»¥é€šè¿‡GradientTape.watch()æ–¹æ³•ä¸´æ—¶åŠ å…¥è·Ÿè¸ªæ¢¯åº¦ä¿¡æ¯çš„åˆ—è¡¨ï¼Œä»è€Œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½**ã€‚    

## 4. æ•°æ®æ“ä½œ

### 4.1 åˆ›å»ºå¼ é‡

```py
tf.constant() tf.convert_to_tensor() è‡ªåŠ¨çš„æŠŠNumpy æ•°ç»„æˆ–è€…Pythonåˆ—è¡¨æ•°æ®ç±»å‹è½¬åŒ–ä¸ºTensor ç±»å‹
tf.zeros()å’Œtf.ones()å³å¯åˆ›å»ºä»»æ„å½¢çŠ¶ï¼Œä¸”å†…å®¹å…¨0 æˆ–å…¨1 çš„å¼ é‡  
tf.zeros_like, tf.ones_like å¯ä»¥æ–¹ä¾¿åœ°æ–°å»ºä¸æŸä¸ªå¼ é‡shape ä¸€è‡´ï¼Œä¸”å†…å®¹ä¸ºå…¨0 æˆ–å…¨1 çš„å¼ é‡ã€‚  
tf.fill(shape, value)å¯ä»¥åˆ›å»ºå…¨ä¸ºè‡ªå®šä¹‰æ•°å€¼value çš„å¼ é‡ï¼Œå½¢çŠ¶ç”±shape å‚æ•°æŒ‡å®šã€‚   
tf.random.normal(shape, mean=0.0, stddev=1.0)å¯ä»¥åˆ›å»ºå½¢çŠ¶ä¸ºshapeï¼Œå‡å€¼ä¸ºmeanï¼Œæ ‡å‡†å·®ä¸ºstddev çš„æ­£æ€åˆ†å¸ƒğ’©(mean, stddev2)  
tf.random.truncated_normal([784, 256], stddev=0.1)ä½¿ç”¨æˆªæ–­çš„æ­£å¤ªåˆ†å¸ƒåˆå§‹åŒ–æƒå€¼å¼ é‡
tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)å¯ä»¥åˆ›å»ºé‡‡æ ·è‡ª
[minval, maxval)åŒºé—´çš„å‡åŒ€åˆ†å¸ƒçš„å¼ é‡,å¦‚æœéœ€è¦å‡åŒ€é‡‡æ ·æ•´å½¢ç±»å‹çš„æ•°æ®ï¼Œå¿…é¡»æŒ‡å®šé‡‡æ ·åŒºé—´çš„æœ€å¤§å€¼maxval å‚æ•°ï¼ŒåŒæ—¶æŒ‡å®šæ•°æ®ç±»å‹ä¸ºtf.int*å‹  
eg:tf.random.uniform([2,2]) # åˆ›å»ºé‡‡æ ·è‡ª[0,1)å‡åŒ€åˆ†å¸ƒçš„çŸ©é˜µ
tf.range(limit, delta=1)å¯ä»¥åˆ›å»º[0, limit)ä¹‹é—´ï¼Œæ­¥é•¿ä¸ºdelta çš„æ•´å‹åºåˆ—ï¼Œä¸åŒ…å«limit æœ¬èº«
tf.range(start, limit, delta=1)å¯ä»¥åˆ›å»º[start, limit)ï¼Œæ­¥é•¿ä¸ºdelta çš„åºåˆ—ï¼Œä¸åŒ…å«limit æœ¬èº«  
```

### 4.2 ç´¢å¼•å’Œåˆ‡ç‰‡

é€šè¿‡ç´¢å¼•ä¸åˆ‡ç‰‡æ“ä½œå¯ä»¥æå–å¼ é‡çš„éƒ¨åˆ†æ•°æ®  

**ç´¢å¼•**ï¼šåœ¨ TensorFlow ä¸­ï¼Œæ”¯æŒæ ‡å‡†ä¸‹æ ‡ç´¢å¼•æ–¹å¼ï¼Œä¹Ÿæ”¯æŒé€šè¿‡é€—å·åˆ†éš”ç´¢å¼•å·çš„ç´¢å¼•æ–¹å¼

```python
x = tf.random.normal([4,32,32,3]) # åˆ›å»º4D å¼ é‡
x[0] # å–ç¬¬ 1 å¼ å›¾ç‰‡çš„æ•°æ®
x[0][1] #å–ç¬¬ 1 å¼ å›¾ç‰‡çš„ç¬¬2 è¡Œ
x[0][1][2] #å–ç¬¬ 1 å¼ å›¾ç‰‡ï¼Œç¬¬2 è¡Œï¼Œç¬¬3 åˆ—çš„æ•°æ®
x[2][1][0][1] #å–ç¬¬ 3 å¼ å›¾ç‰‡ï¼Œç¬¬2 è¡Œï¼Œç¬¬1 åˆ—çš„åƒç´ ï¼ŒB é€šé“(ç¬¬2 ä¸ªé€šé“)é¢œè‰²å¼ºåº¦å€¼
x[1,9,2]  # å–ç¬¬ 2 å¼ å›¾ç‰‡ï¼Œç¬¬10 è¡Œï¼Œç¬¬3 åˆ—çš„æ•°æ®
```

**åˆ‡ç‰‡**ï¼šé€šè¿‡start: end: stepåˆ‡ç‰‡æ–¹å¼å¯ä»¥æ–¹ä¾¿åœ°æå–ä¸€æ®µæ•°æ®ï¼Œå…¶ä¸­start ä¸ºå¼€å§‹è¯»å–ä½ç½®çš„ç´¢
å¼•ï¼Œend ä¸ºç»“æŸè¯»å–ä½ç½®çš„ç´¢å¼•(ä¸åŒ…å«end ä½)ï¼Œstep ä¸ºé‡‡æ ·æ­¥é•¿  

```python
x = tf.random.normal([4,32,32,3]) # åˆ›å»º4D å¼ é‡
x[1:3] #è¯»å–ç¬¬2ï¼Œ3å¼ å›¾ç‰‡
# start: end: stepåˆ‡ç‰‡æ–¹å¼æœ‰å¾ˆå¤šç®€å†™æ–¹å¼ï¼Œå…¶ä¸­startã€endã€step 3 ä¸ªå‚æ•°å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©æ€§åœ°çœç•¥ï¼Œå…¨éƒ¨çœç•¥æ—¶å³ä¸º::ï¼Œè¡¨ç¤ºä»æœ€å¼€å§‹è¯»å–åˆ°æœ€æœ«å°¾ï¼Œæ­¥é•¿ä¸º1
# ä»ç¬¬ä¸€ä¸ªå…ƒç´ è¯»å–æ—¶start å¯ä»¥çœç•¥ï¼Œå³start=0 æ˜¯å¯ä»¥çœç•¥ï¼Œå–åˆ°æœ€åä¸€ä¸ªå…ƒç´ æ—¶end å¯ä»¥çœç•¥ï¼Œæ­¥é•¿ä¸º1 æ—¶step å¯ä»¥çœç•¥  
x[0,::] # è¡¨ç¤ºè¯»å–ç¬¬1 å¼ å›¾ç‰‡çš„æ‰€æœ‰è¡Œï¼Œå…¶ä¸­::è¡¨ç¤ºåœ¨è¡Œç»´åº¦ä¸Šè¯»å–æ‰€æœ‰è¡Œï¼Œå®ƒç­‰ä»·äºx[0]çš„å†™æ³•
#ä¸ºäº†æ›´åŠ ç®€æ´ï¼Œ::å¯ä»¥ç®€å†™ä¸ºå•ä¸ªå†’å·:
x[:,0:28:2,0:28:2,:] #è¯»å–æ‰€æœ‰å›¾ç‰‡ã€éš”è¡Œé‡‡æ ·ã€éš”åˆ—é‡‡æ ·ï¼Œã€è¯»å–æ‰€æœ‰é€šé“æ•°æ®ï¼Œç›¸å½“äºåœ¨å›¾ç‰‡çš„é«˜å®½ä¸Šå„ç¼©æ”¾è‡³åŸæ¥çš„50%
#step å¯ä»¥ä¸ºè´Ÿæ•°ï¼Œè€ƒè™‘æœ€ç‰¹æ®Šçš„ä¸€ç§ä¾‹å­ï¼Œå½“step = âˆ’1æ—¶ï¼Œstart: end: âˆ’1è¡¨ç¤ºä»start å¼€å§‹ï¼Œé€†åºè¯»å–è‡³end ç»“æŸ(ä¸åŒ…å«end)ï¼Œç´¢å¼•å·ğ‘’ğ‘›ğ‘‘ â‰¤ ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡
x[::-1] # é€†åºå…¨éƒ¨å…ƒç´ 
x[0,::-2,::-2] # è¡Œã€åˆ—é€†åºé—´éš”é‡‡æ ·
# å½“å¼ é‡çš„ç»´åº¦æ•°é‡è¾ƒå¤šæ—¶ï¼Œä¸éœ€è¦é‡‡æ ·çš„ç»´åº¦ä¸€èˆ¬ç”¨å•å†’å·:è¡¨ç¤ºé‡‡æ ·æ‰€æœ‰å…ƒç´ ï¼Œæ­¤æ—¶æœ‰å¯èƒ½å‡ºç°å¤§é‡çš„:å‡ºç°ã€‚ç»§ç»­è€ƒè™‘[4,32,32,3]çš„å›¾ç‰‡å¼ é‡ï¼Œå½“éœ€è¦è¯»å–G é€šé“ä¸Šçš„æ•°æ®æ—¶ï¼Œå‰é¢æ‰€æœ‰ç»´åº¦å…¨éƒ¨æå–ï¼Œæ­¤æ—¶éœ€è¦å†™ä¸º 
x[:,:,:,1]
# ä¸ºäº†é¿å…å‡ºç°åƒ [: , : , : ,1]è¿™æ ·è¿‡å¤šå†’å·çš„æƒ…å†µï¼Œå¯ä»¥ä½¿ç”¨â‹¯ç¬¦å·è¡¨ç¤ºå–å¤šä¸ªç»´åº¦ä¸Šæ‰€æœ‰çš„æ•°æ®ï¼Œå…¶ä¸­ç»´åº¦çš„æ•°é‡éœ€æ ¹æ®è§„åˆ™è‡ªåŠ¨æ¨æ–­ï¼šå½“åˆ‡ç‰‡æ–¹å¼å‡ºç°â‹¯ç¬¦å·æ—¶ï¼Œâ‹¯ç¬¦å·å·¦è¾¹çš„ç»´åº¦å°†è‡ªåŠ¨å¯¹é½åˆ°æœ€å·¦è¾¹ï¼Œâ‹¯ç¬¦å·å³è¾¹çš„ç»´åº¦å°†è‡ªåŠ¨å¯¹é½åˆ°æœ€å³è¾¹ï¼Œæ­¤æ—¶ç³»ç»Ÿå†è‡ªåŠ¨æ¨æ–­â‹¯ç¬¦å·ä»£è¡¨çš„ç»´åº¦æ•°é‡
x[0:2,...,1:] # é«˜å®½ç»´åº¦å…¨éƒ¨é‡‡é›†
x[2:,...] # é«˜ã€å®½ã€é€šé“ç»´åº¦å…¨éƒ¨é‡‡é›†ï¼Œç­‰ä»·äºx[2:]
x[...,:2] # æ‰€æœ‰æ ·æœ¬ï¼Œæ‰€æœ‰é«˜ã€å®½çš„å‰2 ä¸ªé€šé“
```

### 4.3 ç»´åº¦å˜æ¢

åŸºæœ¬çš„ç»´åº¦å˜æ¢æ“ä½œå‡½æ•°åŒ…å«äº†æ”¹å˜è§†å›¾reshapeã€æ’å…¥æ–°ç»´åº¦expand_dimsï¼Œåˆ é™¤ç»´åº¦squeezeã€äº¤æ¢ç»´åº¦transposeã€å¤åˆ¶æ•°æ®tile ç­‰å‡½æ•°

**reshape**:reshapeåªä¼šæ”¹å˜æ•°æ®è§†å›¾ï¼ˆå³æ•°æ®è¯»å–æ–¹å¼ï¼‰ï¼Œä¸ä¼šæ”¹å˜æ•°æ®çš„å­˜å‚¨ï¼Œåœ¨ TensorFlow ä¸­ï¼Œå¯ä»¥é€šè¿‡å¼ é‡çš„ndim å’Œshape æˆå‘˜å±æ€§è·å¾—å¼ é‡çš„ç»´åº¦æ•°å’Œå½¢çŠ¶ã€‚  

å‚æ•°âˆ’1è¡¨ç¤ºå½“å‰è½´ä¸Šé•¿åº¦éœ€è¦æ ¹æ®å¼ é‡æ€»å…ƒç´ ä¸å˜çš„æ³•åˆ™è‡ªåŠ¨æ¨å¯¼ï¼Œä»è€Œæ–¹ä¾¿ç”¨æˆ·ä¹¦å†™

**expand_dims**ï¼šå¢åŠ ä¸€ä¸ªé•¿åº¦ä¸º1 çš„ç»´åº¦ç›¸å½“äºç»™åŸæœ‰çš„æ•°æ®æ·»åŠ ä¸€ä¸ªæ–°ç»´åº¦çš„æ¦‚å¿µï¼Œç»´åº¦é•¿åº¦ä¸º1ï¼Œæ•…æ•°æ®å¹¶ä¸éœ€è¦æ”¹å˜ï¼Œä»…ä»…æ˜¯æ”¹å˜æ•°æ®çš„ç†è§£æ–¹å¼ï¼Œå› æ­¤å®ƒå…¶å®å¯ä»¥ç†è§£ä¸ºæ”¹å˜è§†å›¾çš„ä¸€ç§ç‰¹æ®Šæ–¹å¼  

```python
x = tf.random.uniform([28,28],maxval=10,dtype=tf.int32)
#é€šè¿‡tf.expand_dims(x, axis)å¯åœ¨æŒ‡å®šçš„axis è½´å‰å¯ä»¥æ’å…¥ä¸€ä¸ªæ–°çš„ç»´åº¦
x = tf.expand_dims(x,axis=2)
```

tf.expand_dims çš„axis ä¸ºæ­£æ—¶ï¼Œè¡¨ç¤ºåœ¨å½“å‰ç»´åº¦ä¹‹å‰æ’å…¥ä¸€ä¸ªæ–°ç»´åº¦ï¼›ä¸ºè´Ÿæ—¶ï¼Œè¡¨ç¤ºä»åå¾€å‰æ•°ï¼Œåœ¨æ‰€åœ¨ä½ç½®æ’å…¥ä¸€ä¸ªç»´åº¦

**squeeze**ï¼šæ˜¯å¢åŠ ç»´åº¦çš„é€†æ“ä½œï¼Œä¸å¢åŠ ç»´åº¦ä¸€æ ·ï¼Œåˆ é™¤ç»´åº¦åªèƒ½åˆ é™¤é•¿åº¦ä¸º1 çš„ç»´åº¦ï¼Œä¹Ÿä¸ä¼šæ”¹å˜å¼ é‡çš„å­˜å‚¨ã€‚

é€šè¿‡tf.squeeze(x, axis)å‡½æ•°ï¼Œaxis å‚æ•°ä¸ºå¾…åˆ é™¤çš„ç»´åº¦çš„ç´¢å¼•å·ï¼Œå¦‚æœä¸æŒ‡å®šç»´åº¦å‚æ•°axisï¼Œå³tf.squeeze(x)ï¼Œé‚£ä¹ˆå®ƒä¼šé»˜è®¤åˆ é™¤æ‰€æœ‰é•¿åº¦ä¸º1 çš„ç»´åº¦

**transpose**ï¼šæ”¹å˜è§†å›¾ã€å¢åˆ ç»´åº¦éƒ½ä¸ä¼šå½±å“å¼ é‡çš„å­˜å‚¨ï¼Œäº¤æ¢ç»´åº¦(Transpose)ä¼šè°ƒæ•´å­˜å‚¨é¡ºåºã€‚äº¤æ¢ç»´åº¦æ“ä½œæ˜¯éå¸¸å¸¸è§çš„ï¼Œæ¯”å¦‚åœ¨TensorFlow ä¸­ï¼Œå›¾ç‰‡å¼ é‡çš„é»˜è®¤å­˜å‚¨æ ¼å¼æ˜¯é€šé“åè¡Œæ ¼å¼ï¼š[ğ‘, â„, , ğ‘]ï¼Œä½†æ˜¯éƒ¨åˆ†åº“çš„å›¾ç‰‡æ ¼å¼æ˜¯é€šé“å…ˆè¡Œæ ¼å¼ï¼š[ğ‘, ğ‘, â„, ]ï¼Œå› æ­¤éœ€è¦å®Œæˆ[ğ‘, â„, , ğ‘]åˆ°[ğ‘, ğ‘, â„, ]ç»´åº¦äº¤æ¢è¿ç®—ï¼Œæ­¤æ—¶è‹¥ç®€å•çš„ä½¿ç”¨æ”¹å˜è§†å›¾å‡½æ•°reshapeï¼Œåˆ™æ–°è§†å›¾çš„å­˜å‚¨æ–¹å¼éœ€è¦æ”¹å˜ï¼Œå› æ­¤ä½¿ç”¨æ”¹å˜è§†å›¾å‡½æ•°æ˜¯ä¸åˆæ³•çš„ã€‚  

tf.transpose(x, perm)å‡½æ•°å®Œæˆç»´åº¦äº¤æ¢æ“ä½œï¼Œå…¶ä¸­å‚æ•°permè¡¨ç¤ºæ–°ç»´åº¦çš„é¡ºåºList

```python
x = tf.random.normal([2,32,32,3])
tf.transpose(x,perm=[0,3,1,2]) # äº¤æ¢ç»´åº¦
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡tf.transpose å®Œæˆç»´åº¦äº¤æ¢åï¼Œå¼ é‡çš„å­˜å‚¨é¡ºåºå·²ç»æ”¹å˜ï¼Œè§†å›¾ä¹Ÿéšä¹‹æ”¹å˜ï¼Œåç»­çš„æ‰€æœ‰æ“ä½œå¿…é¡»åŸºäºæ–°çš„å­˜ç»­é¡ºåºå’Œè§†å›¾è¿›è¡Œ  

**tile** :å½“é€šè¿‡å¢åŠ ç»´åº¦æ“ä½œæ’å…¥æ–°ç»´åº¦åï¼Œå¯èƒ½å¸Œæœ›åœ¨æ–°çš„ç»´åº¦ä¸Šé¢å¤åˆ¶è‹¥å¹²ä»½æ•°æ®ï¼Œæ»¡è¶³åç»­ç®—æ³•çš„æ ¼å¼è¦æ±‚ã€‚å¯ä»¥é€šè¿‡tf.tile(x, multiples)å‡½æ•°å®Œæˆæ•°æ®åœ¨æŒ‡å®šç»´åº¦ä¸Šçš„å¤åˆ¶æ“ä½œï¼Œmultiples åˆ†åˆ«æŒ‡å®šäº†æ¯ä¸ªç»´åº¦ä¸Šé¢çš„å¤åˆ¶å€æ•°ï¼Œå¯¹åº”ä½ç½®ä¸º1 è¡¨æ˜ä¸å¤åˆ¶ï¼Œä¸º2 è¡¨æ˜æ–°é•¿åº¦ä¸ºåŸæ¥é•¿åº¦çš„2 å€ï¼Œå³æ•°æ®å¤åˆ¶ä¸€ä»½ï¼Œä»¥æ­¤ç±»æ¨

```python
b = tf.constant([1,2])
b = tf.expand_dims(b, axis=0)
b = tf.tile(b, multiples=[2,1]) # æ ·æœ¬ç»´åº¦ä¸Šå¤åˆ¶ä¸€ä»½
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œtf.tile ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å¼ é‡æ¥ä¿å­˜å¤åˆ¶åçš„å¼ é‡ï¼Œç”±äºå¤åˆ¶æ“ä½œæ¶‰åŠå¤§é‡æ•°æ®çš„è¯»å†™IO è¿ç®—ï¼Œè®¡ç®—ä»£ä»·ç›¸å¯¹è¾ƒé«˜ã€‚  

**Broadcasting**:Broadcasting ç§°ä¸ºå¹¿æ’­æœºåˆ¶(æˆ–è‡ªåŠ¨æ‰©å±•æœºåˆ¶)ï¼Œå®ƒæ˜¯ä¸€ç§è½»é‡çº§çš„å¼ é‡å¤åˆ¶æ‰‹æ®µï¼Œåœ¨é€»è¾‘ä¸Šæ‰©å±•å¼ é‡æ•°æ®çš„å½¢çŠ¶ï¼Œä½†æ˜¯åªä¼šåœ¨éœ€è¦æ—¶æ‰ä¼šæ‰§è¡Œå®é™…å­˜å‚¨å¤åˆ¶æ“ä½œã€‚å¯¹äºå¤§éƒ¨åˆ†åœºæ™¯ï¼ŒBroadcasting æœºåˆ¶éƒ½èƒ½é€šè¿‡ä¼˜åŒ–æ‰‹æ®µé¿å…å®é™…å¤åˆ¶æ•°æ®è€Œå®Œæˆé€»è¾‘è¿ç®—ï¼Œä»è€Œç›¸å¯¹äºtf.tile å‡½æ•°ï¼Œå‡å°‘äº†å¤§é‡è®¡ç®—ä»£ä»·ã€‚  

å¯¹äºæ‰€æœ‰é•¿åº¦ä¸º1 çš„ç»´åº¦ï¼ŒBroadcasting çš„æ•ˆæœå’Œtf.tile ä¸€æ ·ï¼Œéƒ½èƒ½åœ¨æ­¤ç»´åº¦ä¸Šé€»è¾‘å¤åˆ¶æ•°æ®è‹¥å¹²ä»½ï¼ŒåŒºåˆ«åœ¨äºtf.tile ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œæ‰§è¡Œå¤åˆ¶IO æ“ä½œï¼Œå¹¶ä¿å­˜å¤åˆ¶åçš„å¼ é‡æ•°æ®ï¼Œè€ŒBroadcasting å¹¶ä¸ä¼šç«‹å³å¤åˆ¶æ•°æ®ï¼Œå®ƒä¼šåœ¨é€»è¾‘ä¸Šæ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼Œä½¿å¾—è§†å›¾ä¸Šå˜æˆäº†å¤åˆ¶åçš„å½¢çŠ¶ã€‚Broadcasting ä¼šé€šè¿‡æ·±åº¦å­¦ä¹ æ¡†æ¶çš„ä¼˜åŒ–æ‰‹æ®µé¿å…å®é™…å¤åˆ¶æ•°æ®è€Œå®Œæˆé€»è¾‘è¿ç®—ï¼Œè‡³äºæ€ä¹ˆå®ç°çš„ç”¨æˆ·ä¸å¿…å…³å¿ƒï¼Œå¯¹äºç”¨æˆ·æ¥è¯´ï¼ŒBroadcasting å’Œtf.tile å¤
åˆ¶çš„æœ€ç»ˆæ•ˆæœæ˜¯ä¸€æ ·çš„ï¼Œæ“ä½œå¯¹ç”¨æˆ·é€æ˜ï¼Œä½†æ˜¯Broadcasting æœºåˆ¶èŠ‚çœäº†å¤§é‡è®¡ç®—èµ„æºï¼Œå»ºè®®åœ¨è¿ç®—è¿‡ç¨‹ä¸­å°½å¯èƒ½åœ°åˆ©ç”¨Broadcasting æœºåˆ¶æé«˜è®¡ç®—æ•ˆç‡

tf.broadcast_to(x, new_shape)

æ“ä½œç¬¦+åœ¨é‡åˆ°shape ä¸ä¸€è‡´çš„2 ä¸ªå¼ é‡æ—¶ï¼Œä¼šè‡ªåŠ¨è€ƒè™‘å°†2 ä¸ªå¼ é‡è‡ªåŠ¨æ‰©å±•åˆ°ä¸€è‡´çš„shapeï¼Œç„¶åå†è°ƒç”¨tf.add å®Œæˆå¼ é‡ç›¸åŠ è¿ç®—

### 4.4 æ•°å­¦è¿ç®—

**åŠ ã€å‡ã€ä¹˜ã€é™¤**æ˜¯æœ€åŸºæœ¬çš„æ•°å­¦è¿ç®—ï¼Œåˆ†åˆ«é€šè¿‡tf.add, tf.subtract, tf.multiply, tf.divideå‡½æ•°å®ç°ï¼ŒTensorFlow å·²ç»é‡è½½äº†+ã€âˆ’ ã€âˆ— ã€/è¿ç®—ç¬¦ï¼Œä¸€èˆ¬æ¨èç›´æ¥ä½¿ç”¨è¿ç®—ç¬¦æ¥å®ŒæˆåŠ ã€å‡ã€ä¹˜ã€é™¤è¿ç®—

æ•´é™¤å’Œä½™é™¤ä¹Ÿæ˜¯å¸¸è§çš„è¿ç®—ä¹‹ä¸€ï¼Œåˆ†åˆ«é€šè¿‡//å’Œ%è¿ç®—ç¬¦å®ç°ã€‚

**ä¹˜æ–¹è¿ç®—**ï¼š

tf.pow(x, a)å¯ä»¥æ–¹ä¾¿åœ°å®Œæˆğ‘¦ = x^ğ‘çš„ä¹˜æ–¹è¿ç®—ï¼Œä¹Ÿå¯ä»¥é€šè¿‡è¿ç®—ç¬¦**å®ç° xâˆ—âˆ— ğ‘è¿ç®—  

è®¾ç½®æŒ‡æ•°ä¸º1/ğ‘å½¢å¼ï¼Œå³å¯å®ç°æ ¹å·è¿ç®—ã€‚  

```python
x=tf.constant([1.,4.,9.])
x**(0.5) #å¹³æ–¹æ ¹
```

å¯¹äºå¸¸è§çš„å¹³æ–¹å’Œå¹³æ–¹æ ¹è¿ç®—ï¼Œå¯ä»¥ä½¿ç”¨tf.square(x)å’Œtf.sqrt(x)å®ç°

**æŒ‡æ•°å’Œå¯¹æ•°è¿ç®—**ï¼š

tf.pow(a, x)æˆ–è€…**è¿ç®—ç¬¦ä¹Ÿå¯ä»¥æ–¹ä¾¿åœ°å®ç°æŒ‡æ•°è¿ç®—ğ‘^ğ‘¥

tf.exp(x)å®ç°è‡ªç„¶æŒ‡æ•°e^ğ‘¥

tf.math.log(x)å®ç°è‡ªç„¶å¯¹æ•°log_e(x)

å¦‚æœå¸Œæœ›è®¡ç®—å…¶å®ƒåº•æ•°çš„å¯¹æ•°ï¼Œå¯ä»¥æ ¹æ®å¯¹æ•°çš„æ¢åº•å…¬å¼é—´æ¥åœ°é€šè¿‡tf.math.log(x)å®ç°ã€‚å¦‚log_10(x)å¯ä»¥å†™ä¸º

tf.math.log(x)/tf.math.log(10.) # æ¢åº•å…¬å¼

**çŸ©é˜µç›¸ä¹˜è¿ç®—**ï¼š

é€šè¿‡@è¿ç®—ç¬¦å¯ä»¥æ–¹ä¾¿çš„å®ç°çŸ©é˜µç›¸ä¹˜ï¼Œè¿˜å¯ä»¥é€šè¿‡tf.matmul(a, b)å‡½æ•°å®ç°ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒTensorFlow ä¸­çš„çŸ©é˜µç›¸ä¹˜å¯ä»¥ä½¿ç”¨æ‰¹é‡æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯å¼ é‡ğ‘¨å’Œğ‘©çš„ç»´åº¦æ•°å¯ä»¥å¤§äº2ã€‚å½“å¼ é‡ğ‘¨å’Œğ‘©ç»´åº¦æ•°å¤§äº2 æ—¶ï¼ŒTensorFlow ä¼šé€‰æ‹©ğ‘¨å’Œğ‘©çš„æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡ŒçŸ©é˜µç›¸ä¹˜ï¼Œå‰é¢æ‰€æœ‰çš„ç»´åº¦éƒ½è§†ä½œBatch ç»´åº¦ã€‚

```python
a = tf.random.normal([4,3,28,32])
b = tf.random.normal([4,3,32,2])
a@b # æ‰¹é‡å½¢å¼çš„çŸ©é˜µç›¸ä¹˜,shape=(4, 3, 28, 2)
#çŸ©é˜µç›¸ä¹˜å‡½æ•°åŒæ ·æ”¯æŒè‡ªåŠ¨Broadcasting æœºåˆ¶
a = tf.random.normal([4,28,32])
b = tf.random.normal([32,16])
tf.matmul(a,b) #å…ˆè‡ªåŠ¨æ‰©å±•å†çŸ©é˜µç›¸ä¹˜ï¼Œshape=(4,28,16)
```

### 4.5 åˆå¹¶å’Œåˆ†å‰²

**åˆå¹¶**ï¼šå¼ é‡çš„åˆå¹¶å¯ä»¥ä½¿ç”¨æ‹¼æ¥(Concatenate)å’Œå †å (Stack)æ“ä½œå®ç°ï¼Œæ‹¼æ¥æ“ä½œå¹¶ä¸ä¼šäº§ç”Ÿæ–°
çš„ç»´åº¦ï¼Œä»…åœ¨ç°æœ‰çš„ç»´åº¦ä¸Šåˆå¹¶ï¼Œè€Œå †å ä¼šåˆ›å»ºæ–°ç»´åº¦ã€‚

> tf.concat(tensors, axis)å‡½æ•°æ‹¼æ¥å¼ é‡å…¶ä¸­å‚æ•°tensors ä¿å­˜äº†æ‰€æœ‰éœ€è¦åˆå¹¶çš„å¼ é‡Listï¼Œaxis å‚æ•°æŒ‡å®šéœ€è¦åˆå¹¶çš„ç»´åº¦ç´¢å¼•,ä»è¯­æ³•ä¸Šæ¥è¯´ï¼Œæ‹¼æ¥åˆå¹¶æ“ä½œå¯ä»¥åœ¨ä»»æ„çš„ç»´åº¦ä¸Šè¿›è¡Œï¼Œå”¯ä¸€çš„çº¦æŸæ˜¯éåˆå¹¶ç»´åº¦çš„é•¿åº¦å¿…é¡»ä¸€è‡´
>
> tf.stack(tensors, axis)å¯ä»¥å †å æ–¹å¼åˆå¹¶å¤šä¸ªå¼ é‡ï¼Œé€šè¿‡tensors åˆ—è¡¨è¡¨ç¤ºï¼Œå‚æ•°axis æŒ‡å®šæ–°ç»´åº¦æ’å…¥çš„ä½ç½®ï¼Œaxis çš„ç”¨æ³•ä¸tf.expand_dims çš„ä¸€è‡´ï¼Œå½“axis â‰¥ 0æ—¶ï¼Œåœ¨axisä¹‹å‰æ’å…¥ï¼›å½“axis < 0æ—¶ï¼Œåœ¨axis ä¹‹åæ’å…¥æ–°ç»´åº¦ã€‚éœ€è¦æ‰€æœ‰å¾…åˆå¹¶çš„å¼ é‡shape å®Œå…¨ä¸€è‡´æ‰å¯åˆå¹¶

```python
a = tf.random.normal([4,35,8]) # æ¨¡æ‹Ÿæˆç»©å†ŒA
b = tf.random.normal([6,35,8]) # æ¨¡æ‹Ÿæˆç»©å†ŒB
tf.concat([a,b],axis=0) # æ‹¼æ¥åˆå¹¶æˆç»©å†Œ
a = tf.random.normal([35,8])
b = tf.random.normal([35,8])
tf.stack([a,b],axis=0) # å †å åˆå¹¶ä¸º2 ä¸ªç­çº§ï¼Œç­çº§ç»´åº¦æ’å…¥åœ¨æœ€å‰
tf.stack([a,b],axis=-1) # åœ¨æœ«å°¾æ’å…¥ç­çº§ç»´åº¦
```

**åˆ†å‰²**ï¼šå°†ä¸€ä¸ªå¼ é‡åˆ†æ‹†ä¸ºå¤šä¸ªå¼ é‡

> tf.split(x, num_or_size_splits, axis)å¯ä»¥å®Œæˆå¼ é‡çš„åˆ†å‰²æ“ä½œ
>
> x å‚æ•°ï¼šå¾…åˆ†å‰²å¼ é‡ã€‚
> num_or_size_splits å‚æ•°ï¼šåˆ‡å‰²æ–¹æ¡ˆã€‚å½“num_or_size_splits ä¸ºå•ä¸ªæ•°å€¼æ—¶ï¼Œå¦‚10ï¼Œè¡¨ç¤ºç­‰é•¿åˆ‡å‰²ä¸º10 ä»½ï¼›å½“num_or_size_splits ä¸ºList æ—¶ï¼ŒList çš„æ¯ä¸ªå…ƒç´ è¡¨ç¤ºæ¯ä»½çš„é•¿åº¦ï¼Œå¦‚[2,4,2,2]è¡¨ç¤ºåˆ‡å‰²ä¸º4 ä»½ï¼Œæ¯ä»½çš„é•¿åº¦ä¾æ¬¡æ˜¯2ã€4ã€2ã€2ã€‚
> axis å‚æ•°ï¼šæŒ‡å®šåˆ†å‰²çš„ç»´åº¦ç´¢å¼•å·ã€‚
>
> tf.unstack(x,axis),åœ¨æŸä¸ªç»´åº¦ä¸Šå…¨éƒ¨æŒ‰é•¿åº¦ä¸º1 çš„æ–¹å¼åˆ†å‰²,è¿™ç§æ–¹å¼æ˜¯tf.split çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œåˆ‡å‰²é•¿åº¦å›ºå®šä¸º1ï¼Œåªéœ€è¦æŒ‡å®šåˆ‡å‰²ç»´åº¦
> çš„ç´¢å¼•å·å³å¯

```python
x = tf.random.normal([10,35,8])
# ç­‰é•¿åˆ‡å‰²ä¸º10 ä»½
result = tf.split(x, num_or_size_splits=10, axis=0)
# è‡ªå®šä¹‰é•¿åº¦çš„åˆ‡å‰²ï¼Œåˆ‡å‰²ä¸º4 ä»½ï¼Œè¿”å›4 ä¸ªå¼ é‡çš„åˆ—è¡¨result
result = tf.split(x, num_or_size_splits=[4,2,2,2] ,axis=0)
result = tf.unstack(x,axis=0) # Unstack ä¸ºé•¿åº¦ä¸º1 çš„å¼ é‡
```

### 4.6 æ•°æ®ç»Ÿè®¡

**å‘é‡èŒƒæ•°**ï¼šå‘é‡èŒƒæ•°(Vector Norm)æ˜¯è¡¨å¾å‘é‡â€œé•¿åº¦â€çš„ä¸€ç§åº¦é‡æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ¨å¹¿åˆ°å¼ é‡ä¸Šã€‚åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå¸¸ç”¨æ¥è¡¨ç¤ºå¼ é‡çš„æƒå€¼å¤§å°ï¼Œæ¢¯åº¦å¤§å°ç­‰ã€‚å¸¸ç”¨çš„å‘é‡èŒƒæ•°æœ‰

* L1 èŒƒæ•°ï¼Œå®šä¹‰ä¸ºå‘é‡ğ’™çš„æ‰€æœ‰å…ƒç´ ç»å¯¹å€¼ä¹‹å’Œ

* L2 èŒƒæ•°ï¼Œå®šä¹‰ä¸ºå‘é‡ğ’™çš„æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œï¼Œå†å¼€æ ¹å·

* âˆ âˆ’èŒƒæ•°ï¼Œå®šä¹‰ä¸ºå‘é‡ğ’™çš„æ‰€æœ‰å…ƒç´ ç»å¯¹å€¼çš„æœ€å¤§å€¼

å¯¹äºçŸ©é˜µå’Œå¼ é‡ï¼ŒåŒæ ·å¯ä»¥åˆ©ç”¨å‘é‡èŒƒæ•°çš„è®¡ç®—å…¬å¼ï¼Œç­‰ä»·äºå°†çŸ©é˜µå’Œå¼ é‡æ‰“å¹³æˆå‘é‡åè®¡ç®—

tf.norm(x, ord)æ±‚è§£å¼ é‡çš„L1ã€L2ã€âˆç­‰èŒƒæ•°ï¼Œå…¶ä¸­å‚æ•°ord æŒ‡å®šä¸º1ã€2 æ—¶è®¡ç®—L1ã€L2 èŒƒæ•°ï¼ŒæŒ‡å®šä¸ºnp.inf æ—¶è®¡ç®—âˆ âˆ’èŒƒæ•°

```python
x = tf.ones([2,2])
tf.norm(x,ord=1) # è®¡ç®—L1 èŒƒæ•°
tf.norm(x,ord=2) # è®¡ç®—L2 èŒƒæ•°
tf.norm(x,ord=np.inf) # è®¡ç®—âˆèŒƒæ•°
```

**æœ€å€¼ã€å‡å€¼ã€å’Œ**:é€šè¿‡ tf.reduce_maxã€tf.reduce_minã€tf.reduce_meanã€tf.reduce_sum å‡½æ•°å¯ä»¥æ±‚è§£å¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šçš„æœ€å¤§ã€æœ€å°ã€å‡å€¼ã€å’Œï¼Œä¹Ÿå¯ä»¥æ±‚å…¨å±€æœ€å¤§ã€æœ€å°ã€å‡å€¼ã€å’Œä¿¡æ¯   

```python
x = tf.random.normal([4,10]) # æ¨¡å‹ç”Ÿæˆæ¦‚ç‡
tf.reduce_max(x,axis=1) # ç»Ÿè®¡æ¦‚ç‡ç»´åº¦ä¸Šçš„æœ€å¤§å€¼
tf.reduce_min(x,axis=1) # ç»Ÿè®¡æ¦‚ç‡ç»´åº¦ä¸Šçš„æœ€å°å€¼
tf.reduce_mean(x,axis=1) # ç»Ÿè®¡æ¦‚ç‡ç»´åº¦ä¸Šçš„å‡å€¼
tf.reduce_sum(x,axis=-1) # æ±‚æœ€åä¸€ä¸ªç»´åº¦çš„å’Œ
```

é™¤äº†å¸Œæœ›è·å–å¼ é‡çš„æœ€å€¼ä¿¡æ¯ï¼Œè¿˜å¸Œæœ›è·å¾—æœ€å€¼æ‰€åœ¨çš„ä½ç½®ç´¢å¼•å·ï¼Œä¾‹å¦‚åˆ†ç±»ä»»åŠ¡çš„æ ‡ç­¾é¢„æµ‹ï¼Œå°±éœ€è¦çŸ¥é“æ¦‚ç‡æœ€å¤§å€¼æ‰€åœ¨çš„ä½ç½®ç´¢å¼•å·ï¼Œä¸€èˆ¬æŠŠè¿™ä¸ªä½ç½®ç´¢å¼•å·ä½œä¸ºé¢„æµ‹ç±»åˆ«  

tf.argmax(x, axis)å’Œtf.argmin(x, axis)å¯ä»¥æ±‚è§£åœ¨axis è½´ä¸Šï¼Œx çš„æœ€å¤§å€¼ã€æœ€å°å€¼æ‰€åœ¨çš„ç´¢å¼•å·

```python
pred = tf.argmax(out, axis=1) # é€‰å–æ¦‚ç‡æœ€å¤§çš„ä½ç½®
pred = tf.argmin(out, axis=1) # é€‰å–æ¦‚ç‡æœ€å°çš„ä½ç½®
```

### 4.7 å¼ é‡æ¯”è¾ƒ

é€šè¿‡tf.equal(a, b)(æˆ–tf.math.equal(a,b)ï¼Œä¸¤è€…ç­‰ä»·)å‡½æ•°å¯ä»¥æ¯”è¾ƒè¿™2 ä¸ªå¼ é‡æ˜¯å¦ç›¸ç­‰

tf.equal()å‡½æ•°è¿”å›å¸ƒå°”ç±»å‹çš„å¼ é‡æ¯”è¾ƒç»“æœï¼Œåªéœ€è¦ç»Ÿè®¡å¼ é‡ä¸­True å…ƒç´ çš„ä¸ªæ•°ï¼Œå³å¯çŸ¥é“é¢„æµ‹æ­£ç¡®çš„ä¸ªæ•°

tf.math.greater : a>b

tf.math.less : a<b

tf.math.greater_equal : a>=b

tf.math.less_equal : a<=b

tf.math.not_equal : a!=b

tf.math.is_nan : a=nan

```python
out = tf.random.normal([100,10])
out = tf.nn.softmax(out, axis=1) # è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡
pred = tf.argmax(out, axis=1) # è®¡ç®—é¢„æµ‹å€¼
y = tf.random.uniform([100],dtype=tf.int64,maxval=10)
out = tf.equal(pred,y) # é¢„æµ‹å€¼ä¸çœŸå®å€¼æ¯”è¾ƒï¼Œè¿”å›å¸ƒå°”ç±»å‹çš„å¼ é‡
out = tf.cast(out, dtype=tf.float32) # å¸ƒå°”å‹è½¬int å‹
correct = tf.reduce_sum(out) # ç»Ÿè®¡True çš„ä¸ªæ•°
```

### 4.8 å¡«å……ä¸å¤åˆ¶

tf.pad(x, paddings)å‡½æ•°å®ç°å¡«å……æ“ä½œ,å‚æ•°paddings æ˜¯åŒ…å«äº†å¤šä¸ª[Left Padding, Right Padding]çš„åµŒå¥—æ–¹æ¡ˆListï¼Œå¦‚[[0,0], [2,1], [1,2]]è¡¨ç¤ºç¬¬ä¸€ä¸ªç»´åº¦ä¸å¡«å……ï¼Œç¬¬äºŒä¸ªç»´åº¦å·¦è¾¹(èµ·å§‹å¤„)å¡«å……ä¸¤ä¸ªå•å…ƒï¼Œå³è¾¹(ç»“æŸå¤„)å¡«å……ä¸€ä¸ªå•å…ƒï¼Œç¬¬ä¸‰ä¸ªç»´åº¦å·¦è¾¹å¡«å……ä¸€ä¸ªå•å…ƒï¼Œå³è¾¹å¡«å……ä¸¤ä¸ªå•å…ƒã€‚

```python
total_words = 10000 # è®¾å®šè¯æ±‡é‡å¤§å°
max_review_len = 80 # æœ€å¤§å¥å­é•¿åº¦
embedding_len = 100 # è¯å‘é‡é•¿åº¦
# åŠ è½½IMDB æ•°æ®é›†
(x_train, y_train), (x_test, y_test) =
keras.datasets.imdb.load_data(num_words=total_words)
# å°†å¥å­å¡«å……æˆ–æˆªæ–­åˆ°ç›¸åŒé•¿åº¦ï¼Œè®¾ç½®ä¸ºæœ«å°¾å¡«å……å’Œæœ«å°¾æˆªæ–­æ–¹å¼
x_train = keras.preprocessing.sequence.pad_sequences(x_train,
maxlen=max_review_len,truncating='post',padding='post')
x_test = keras.preprocessing.sequence.pad_sequences(x_test,
maxlen=max_review_len,truncating='post',padding='post')
print(x_train.shape, x_test.shape) # æ‰“å°ç­‰é•¿çš„å¥å­å¼ é‡å½¢çŠ¶
```

tf.tile å‡½æ•°å¯ä»¥åœ¨ä»»æ„ç»´åº¦å°†æ•°æ®é‡å¤å¤åˆ¶å¤šä»½ï¼Œå¦‚shape ä¸º[4,32,32,3]çš„æ•°æ®ï¼Œå¤åˆ¶æ–¹æ¡ˆä¸ºmultiples=[2,3,3,1]ï¼Œå³é€šé“æ•°æ®ä¸å¤åˆ¶ï¼Œé«˜å’Œå®½æ–¹å‘åˆ†åˆ«å¤åˆ¶2 ä»½ï¼Œå›¾ç‰‡æ•°å†å¤åˆ¶1 ä»½

```python
x = tf.random.normal([4,32,32,3])
tf.tile(x,[2,3,3,1]) # æ•°æ®å¤åˆ¶
```

### 4.9æ•°æ®é™å¹…

tf.maximum(x, a)å®ç°æ•°æ®çš„ä¸‹é™å¹…ï¼Œå³ğ‘¥ âˆˆ [ğ‘, +âˆ)ï¼›

tf.minimum(x, a)å®ç°æ•°æ®çš„ä¸Šé™å¹…ï¼Œå³ğ‘¥ âˆˆ (âˆ’âˆ, ğ‘]ï¼Œ

tf.clip_by_value(x,2,7)å®ç°ä¸Šä¸‹é™å¹…

tf.clip_by_norm(a,5) æŒ‰L2èŒƒæ•°æ–¹å¼è£å‰ª

tf.clip_by_global_norm  ç¼©æ”¾æ•´ä½“ç½‘ç»œæ¢¯åº¦ğ‘¾çš„L2èŒƒæ•°

```
tf.minimum(tf.maximum(x,2),7) # é™å¹…ä¸º2~7
tf.clip_by_value(x,2,7) # é™å¹…ä¸º2~7
```

## ## 4.10 é«˜çº§æ“ä½œ

**tf.gather**ï¼šæ ¹æ®ç´¢å¼•å·æ”¶é›†æ•°æ®ï¼Œé€‚åˆç´¢å¼•å·æ²¡æœ‰è§„åˆ™çš„åœºåˆï¼Œå…¶ä¸­ç´¢å¼•å·å¯ä»¥ä¹±åºæ’åˆ—ï¼Œæ­¤æ—¶æ”¶é›†çš„æ•°æ®ä¹Ÿæ˜¯å¯¹åº”é¡ºåº

```python
x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32) # æˆç»©å†Œå¼ é‡
tf.gather(x,[0,1],axis=0) # åœ¨ç­çº§ç»´åº¦æ”¶é›†ç¬¬1~2 å·ç­çº§æˆç»©å†Œ
tf.gather(x,[0,3,8,11,12,26],axis=1) # æ”¶é›†ç¬¬1,4,9,12,13,27 å·åŒå­¦æˆç»©
tf.gather(x,[2,4],axis=2) # ç¬¬3ï¼Œ5 ç§‘ç›®çš„æˆç»©
```

**tf.gather_nd**ï¼šé€šè¿‡æŒ‡å®šæ¯æ¬¡é‡‡æ ·ç‚¹çš„å¤šç»´åæ ‡æ¥å®ç°é‡‡æ ·å¤šä¸ªç‚¹çš„ç›®çš„

```python
x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32) # æˆç»©å†Œå¼ é‡
tf.gather_nd(x,[[1,1],[2,2],[3,3]])
tf.gather_nd(x,[[1,1,2],[2,2,3],[3,3,4]]) # æ ¹æ®å¤šç»´åº¦åæ ‡æ”¶é›†æ•°æ®
```

**tf.boolean_mask**ï¼šé€šè¿‡ç»™å®šæ©ç (Mask)çš„æ–¹å¼è¿›è¡Œé‡‡æ ·,æ³¨æ„æ©ç çš„é•¿åº¦å¿…é¡»ä¸å¯¹åº”ç»´åº¦çš„é•¿åº¦ä¸€è‡´ï¼Œ

```python
x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32) # æˆç»©å†Œå¼ é‡
tf.boolean_mask(x,mask=[True, False,False,True],axis=0)
x = tf.random.uniform([2,3,8],maxval=100,dtype=tf.int32)
tf.gather_nd(x,[[0,0],[0,1],[1,1],[1,2]]) # å¤šç»´åæ ‡é‡‡é›†
tf.boolean_mask(x,[[True,True,False],[False,True,True]]) # å¤šç»´æ©ç é‡‡æ ·
```

**tf.where**ï¼štf.where(cond, a, b)æ“ä½œå¯ä»¥æ ¹æ®cond æ¡ä»¶çš„çœŸå‡ä»å‚æ•°ğ‘¨æˆ–ğ‘©ä¸­è¯»å–æ•°æ®ï¼Œ

å½“å‚æ•°a=b=None æ—¶ï¼Œå³a å’Œb å‚æ•°ä¸æŒ‡å®šï¼Œtf.where ä¼šè¿”å›cond å¼ é‡ä¸­æ‰€æœ‰True çš„å…ƒç´ çš„ç´¢å¼•åæ ‡ã€‚é‚£ä¹ˆè¿™æœ‰ä»€ä¹ˆç”¨é€”å‘¢ï¼Ÿè€ƒè™‘ä¸€ä¸ªåœºæ™¯ï¼Œæˆ‘ä»¬éœ€è¦æå–å¼ é‡ä¸­æ‰€æœ‰æ­£æ•°çš„æ•°æ®å’Œç´¢å¼•ã€‚é¦–å…ˆæ„é€ å¼ é‡aï¼Œå¹¶é€šè¿‡æ¯”è¾ƒè¿ç®—å¾—åˆ°æ‰€æœ‰æ­£æ•°çš„ä½ç½®æ©ç 

```python
a = tf.ones([3,3]) # æ„é€ a ä¸ºå…¨1 çŸ©é˜µ
b = tf.zeros([3,3]) # æ„é€ b ä¸ºå…¨0 çŸ©é˜µ
cond =tf.constant([[True,False,False],[False,True,False],[True,True,False]])
tf.where(cond,a,b) # æ ¹æ®æ¡ä»¶ä»a,b ä¸­é‡‡æ ·
tf.where(cond) # è·å–cond ä¸­ä¸ºTrue çš„å…ƒç´ ç´¢å¼•

x = tf.random.normal([3,3])
mask=x>0
indices=tf.where(mask) # æå–æ‰€æœ‰å¤§äº0 çš„å…ƒç´ ç´¢å¼•
tf.gather_nd(x,indices) # æå–æ­£æ•°çš„å…ƒç´ å€¼
tf.boolean_mask(x,mask) # é€šè¿‡æ©ç æå–æ­£æ•°çš„å…ƒç´ å€¼
```

**scatter_nd**ï¼štf.scatter_nd(indices, updates, shape)å‡½æ•°å¯ä»¥é«˜æ•ˆåœ°åˆ·æ–°å¼ é‡çš„éƒ¨åˆ†æ•°æ®ï¼Œä½†æ˜¯è¿™ä¸ªå‡½æ•°åªèƒ½åœ¨å…¨0 çš„ç™½æ¿å¼ é‡ä¸Šé¢æ‰§è¡Œåˆ·æ–°æ“ä½œï¼Œå› æ­¤å¯èƒ½éœ€è¦ç»“åˆå…¶å®ƒæ“ä½œæ¥å®ç°ç°æœ‰å¼ é‡çš„æ•°æ®åˆ·æ–°åŠŸèƒ½ã€‚  

ç™½æ¿çš„å½¢çŠ¶é€šè¿‡shape å‚æ•°è¡¨ç¤ºï¼Œéœ€è¦åˆ·æ–°çš„æ•°æ®ç´¢å¼•å·é€šè¿‡indices è¡¨ç¤ºï¼Œæ–°æ•°æ®ä¸ºupdatesã€‚æ ¹æ®indices ç»™å‡ºçš„ç´¢å¼•ä½ç½®å°†updates ä¸­æ–°çš„æ•°æ®ä¾æ¬¡å†™å…¥ç™½æ¿ä¸­ï¼Œå¹¶è¿”å›æ›´æ–°åçš„ç»“æœå¼ é‡  

```python
# æ„é€ éœ€è¦åˆ·æ–°æ•°æ®çš„ä½ç½®å‚æ•°ï¼Œå³ä¸º4ã€3ã€1 å’Œ7 å·ä½ç½®
indices = tf.constant([[4], [3], [1], [7]])
# æ„é€ éœ€è¦å†™å…¥çš„æ•°æ®ï¼Œ4 å·ä½å†™å…¥4.4,3 å·ä½å†™å…¥3.3ï¼Œä»¥æ­¤ç±»æ¨
updates = tf.constant([4.4, 3.3, 1.1, 7.7])
# åœ¨é•¿åº¦ä¸º8 çš„å…¨0 å‘é‡ä¸Šæ ¹æ®indices å†™å…¥updates æ•°æ®
tf.scatter_nd(indices, updates, [8])

# æ„é€ å†™å…¥ä½ç½®ï¼Œå³2 ä¸ªä½ç½®
indices = tf.constant([[1],[3]])
updates = tf.constant([# æ„é€ å†™å…¥æ•°æ®ï¼Œå³2 ä¸ªçŸ©é˜µ
[[5,5,5,5],[6,6,6,6],[7,7,7,7],[8,8,8,8]],
[[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]]
])
# åœ¨shape ä¸º[4,4,4]ç™½æ¿ä¸Šæ ¹æ®indices å†™å…¥updates
tf.scatter_nd(indices,updates,[4,4,4])
```

**meshgrid**ï¼štf.meshgrid å‡½æ•°å¯ä»¥æ–¹ä¾¿åœ°ç”ŸæˆäºŒç»´ç½‘æ ¼çš„é‡‡æ ·ç‚¹åæ ‡ï¼Œæ–¹ä¾¿å¯è§†åŒ–ç­‰åº”ç”¨åœºåˆ

```python
points = [] # ä¿å­˜æ‰€æœ‰ç‚¹çš„åæ ‡åˆ—è¡¨
for x in range(-8,8,100): # å¾ªç¯ç”Ÿæˆx åæ ‡ï¼Œ100 ä¸ªé‡‡æ ·ç‚¹
for y in range(-8,8,100): # å¾ªç¯ç”Ÿæˆy åæ ‡ï¼Œ100 ä¸ªé‡‡æ ·ç‚¹
z = sinc(x,y) # è®¡ç®—æ¯ä¸ªç‚¹(x,y)å¤„çš„sinc å‡½æ•°å€¼
points.append([x,y,z]) # ä¿å­˜é‡‡æ ·ç‚¹

x = tf.linspace(-8.,8,100) # è®¾ç½®x è½´çš„é‡‡æ ·ç‚¹
y = tf.linspace(-8.,8,100) # è®¾ç½®y è½´çš„é‡‡æ ·ç‚¹
x,y = tf.meshgrid(x,y) # ç”Ÿæˆç½‘æ ¼ç‚¹ï¼Œå¹¶å†…éƒ¨æ‹†åˆ†åè¿”å›
x.shape,y.shape # æ‰“å°æ‹†åˆ†åçš„æ‰€æœ‰ç‚¹çš„x,y åæ ‡å¼ é‡shape
# tf.meshgrid ä¼šè¿”å›åœ¨axis=2 ç»´åº¦åˆ‡å‰²åçš„2 ä¸ªå¼ é‡ğ‘¨å’Œğ‘©ï¼Œå…¶ä¸­å¼ é‡ğ‘¨
# åŒ…å«äº†æ‰€æœ‰ç‚¹çš„x åæ ‡ï¼Œğ‘©åŒ…å«äº†æ‰€æœ‰ç‚¹çš„y åæ ‡ï¼Œshape éƒ½ä¸º[100,100]
z = tf.sqrt(x**2+y**2)
z = tf.sin(z)/z # sinc å‡½æ•°å®ç°
```

## 5. ç½‘ç»œæ“ä½œ

TensorFlowåº•å±‚æ¥å£ä¸ºtf.nnï¼Œé«˜å±‚æ¥å£ä¸ºtf.keras.layersï¼Œtf.keras.layers å‘½åç©ºé—´ä¸­æä¾›äº†å¤§é‡å¸¸è§ç½‘ç»œå±‚çš„ç±»ï¼Œå¦‚å…¨è¿æ¥å±‚ã€æ¿€æ´»å‡½æ•°å±‚ã€æ± åŒ–å±‚ã€å·ç§¯å±‚ã€å¾ªç¯ç¥ç»ç½‘ç»œå±‚ç­‰ã€‚

### 5.1 æ¿€æ´»å‡½æ•°

ä¸€èˆ¬æ¥è¯´ï¼Œæ¿€æ´»å‡½æ•°ç±»å¹¶ä¸æ˜¯ä¸»è¦çš„ç½‘ç»œè¿ç®—å±‚ï¼Œä¸è®¡å…¥ç½‘ç»œçš„å±‚æ•°

sigmoid:tf.nn.sigmoid

relu:tf.nn.reluæˆ–layers.ReLU()

leaky_relu:tf.nn.leaky_reluæˆ–layers.LeakyReLU(alpha)

tanh:tf.nn.tanh

```python
x = tf.linspace(-6.,6.,10)
tf.nn.sigmoid(x) # é€šè¿‡Sigmoid å‡½æ•°
tf.nn.relu(x) # é€šè¿‡ReLU æ¿€æ´»å‡½æ•°
layers.ReLU()
tf.nn.leaky_relu(x, alpha=0.1) # é€šè¿‡LeakyReLU æ¿€æ´»å‡½æ•°
layer.LeakyReLU(alpha)
tf.nn.tanh(x) # é€šè¿‡tanh æ¿€æ´»å‡½æ•°
```

### 5.2 åˆ†ç±»å‡½æ•°

tf.nn.softmax  layers.Softmax(axis)

```python
z = tf.constant([2.,1.,0.1])
tf.nn.softmax(z) # é€šè¿‡Softmax å‡½æ•°
ayers.Softmax(axis=-1)
```

### 5.3 è¯¯å·®å‡½æ•°

tf.keras.losses.MSE/keras.losses.MeanSquaredError()ï¼šå‡æ–¹å·®(Mean Squared Errorï¼Œç®€ç§°MSE)è¯¯å·®å‡½æ•°,æ¬§å¼è·ç¦»(å‡†ç¡®åœ°è¯´æ˜¯æ¬§å¼è·ç¦»çš„å¹³æ–¹)

tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)ï¼Œäº¤å‰ç†µï¼Œå…¶ä¸­y_true ä»£è¡¨äº†
One-hot ç¼–ç åçš„çœŸå®æ ‡ç­¾ï¼Œy_pred è¡¨ç¤ºç½‘ç»œçš„é¢„æµ‹å€¼ï¼Œå½“from_logits è®¾ç½®ä¸ºTrue æ—¶ï¼Œ
y_pred è¡¨ç¤ºé¡»ä¸ºæœªç»è¿‡Softmax å‡½æ•°çš„å˜é‡zï¼›å½“from_logits è®¾ç½®ä¸ºFalse æ—¶ï¼Œy_pred è¡¨ç¤º
ä¸ºç»è¿‡Softmax å‡½æ•°çš„è¾“å‡ºã€‚ä¸ºäº†æ•°å€¼è®¡ç®—ç¨³å®šæ€§ï¼Œä¸€èˆ¬è®¾ç½®from_logits ä¸ºTrueï¼Œæ­¤æ—¶
tf.keras.losses.categorical_crossentropy å°†åœ¨å†…éƒ¨è¿›è¡ŒSoftmax å‡½æ•°è®¡ç®—ï¼Œæ‰€ä»¥ä¸éœ€è¦åœ¨æ¨¡å‹
ä¸­æ˜¾å¼è°ƒç”¨Softmax å‡½æ•°

tf.keras.losses.BinaryCrossentropy():äºŒåˆ†ç±»çš„äº¤å‰ç†µæŸå¤±å‡½æ•°

tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec): å¯¹è¾“å…¥çš„logitså…ˆé€šè¿‡sigmoidå‡½æ•°è®¡ç®—ï¼Œå†è®¡ç®—å®ƒä»¬çš„äº¤å‰ç†µï¼Œä½†æ˜¯å®ƒå¯¹äº¤å‰ç†µçš„è®¡ç®—æ–¹å¼è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å¾—çš„ç»“æœä¸è‡³äºæº¢å‡ºã€‚ é€‚ç”¨ï¼šæ¯ä¸ªç±»åˆ«ç›¸äº’ç‹¬ç«‹ä½†äº’ä¸æ’æ–¥çš„æƒ…å†µï¼šä¾‹å¦‚ä¸€å¹…å›¾å¯ä»¥åŒæ—¶åŒ…å«ä¸€æ¡ç‹—å’Œä¸€åªå¤§è±¡ 

```python
o = tf.random.normal([2,10]) # æ„é€ ç½‘ç»œè¾“å‡º
y_onehot = tf.constant([1,3]) # æ„é€ çœŸå®å€¼
y_onehot = tf.one_hot(y_onehot, depth=10)
loss = keras.losses.MSE(y_onehot, o) # è®¡ç®—å‡æ–¹å·®
# åˆ›å»ºMSE ç±»
criteon = keras.losses.MeanSquaredError()
loss = criteon(y_onehot,o) # è®¡ç®—batch å‡æ–¹å·®
# è®¡ç®—äº¤å‰ç†µ
loss = tf.losses.categorical_crossentropy(y_onehot, logits,from_logits=True)
# åˆ›å»ºSoftmax ä¸äº¤å‰ç†µè®¡ç®—ç±»ï¼Œè¾“å‡ºå±‚çš„è¾“å‡ºz æœªä½¿ç”¨softmax
criteon = keras.losses.CategoricalCrossentropy(from_logits=True)
loss = criteon(y_onehot,z) # è®¡ç®—æŸå¤±
# 2 åˆ†ç±»çš„äº¤å‰ç†µæŸå¤±å‡½æ•°
loss = tf.losses.BinaryCrossentropy(y_onehot, logits)
```

### 5.4 ç½‘ç»œå±‚

å…¨è¿æ¥å±‚ã€æ¿€æ´»å‡½æ•°å±‚ã€æ± åŒ–å±‚ã€å·ç§¯å±‚ã€å¾ªç¯ç¥ç»ç½‘ç»œå±‚

#### 5.4.1 å…¨è¿æ¥å±‚

layers.Dense(units, activation)

```python
fc = layers.Dense(256, activation='relu')
fc.kernel # è·å–Dense ç±»çš„æƒå€¼çŸ©é˜µ
fc.bias # è·å–Dense ç±»çš„åç½®å‘é‡
fc.trainable_variables  # è¿”å›å¾…ä¼˜åŒ–å‚æ•°åˆ—è¡¨
fc.variables # è¿”å›æ‰€æœ‰å‚æ•°åˆ—è¡¨
```

#### 5.4.2 å·ç§¯å±‚

å·ç§¯åŒ…å«æ™®é€šå·ç§¯ï¼Œç©ºæ´å·ç§¯å’Œè½¬ç½®å·ç§¯

```python
tf.nn.conv2d(x,w,strides=1,padding=[[0,0],[0,0],[0,0],[0,0]])  
# paddingå‚æ•°ä¸º[[0,0],[ä¸Š,ä¸‹],[å·¦,å³],[0,0]]ï¼Œæˆ–è€…è®¾ç½®ä¸º'SAME'ã€â€˜VALIDâ€™
layers.Conv2D(4,kernel_size=3,strides=1,padding='SAME')
# 3 Ã— 4å¤§å°çš„å·ç§¯æ ¸ï¼Œç«–ç›´æ–¹å‘ç§»åŠ¨æ­¥é•¿ğ‘ â„ = 2ï¼Œæ°´å¹³æ–¹å‘ç§»åŠ¨æ­¥é•¿ğ‘ ğ‘¤ =1 
layers.Conv2D(4,kernel_size=(3,4),strides=(2,1),padding='SAME') 
# å½“dilation_rate å‚æ•°è®¾ç½®ä¸ºé»˜è®¤å€¼1 æ—¶ï¼Œä½¿ç”¨æ™®é€šå·ç§¯æ–¹å¼è¿›è¡Œè¿ç®—ï¼›å½“dilation_rate å‚æ•°å¤§äº1 æ—¶ï¼Œé‡‡æ ·ç©º# æ´å·ç§¯æ–¹å¼è¿›è¡Œè®¡ç®—ã€‚
layers.Conv2D(1,kernel_size=3,strides=1,dilation_rate=2)  ç©ºæ´å·ç§¯ï¼Œ1 ä¸ª3x3 çš„å·ç§¯æ ¸
# æ™®é€šå·ç§¯çš„è¾“å‡ºä½œä¸ºè½¬ç½®å·ç§¯çš„è¾“å…¥ï¼Œè¿›è¡Œè½¬ç½®å·ç§¯è¿ç®—,è¾“å‡ºçš„é«˜å®½ä¸º5x5
tf.nn.conv2d_transpose(out, w, strides=2,padding='VALID',output_shape=[1,5,5,1])
layers.Conv2DTranspose(1,kernel_size=3,strides=1,padding='VALID')
```

#### 5.4.3 æ± åŒ–å±‚

```python
layers.GlobalAveragePooling2D()
layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same')
layers.AveragePool2D(pool_size=[2, 2], strides=2, padding='same')
```

#### 5.4.4 BNå±‚

```python
network = Sequential([ # ç½‘ç»œå®¹å™¨
layers.Conv2D(6,kernel_size=3,strides=1),
# æ’å…¥BN å±‚
layers.BatchNormalization(),
layers.MaxPooling2D(pool_size=2,strides=2),
layers.ReLU(),
layers.Conv2D(16,kernel_size=3,strides=1),
# æ’å…¥BN å±‚
layers.BatchNormalization(),
layers.MaxPooling2D(pool_size=2,strides=2),
layers.ReLU(),
layers.Flatten(),
layers.Dense(120, activation='relu'),
# æ­¤å¤„ä¹Ÿå¯ä»¥æ’å…¥BN å±‚
layers.Dense(84, activation='relu'),
# æ­¤å¤„ä¹Ÿå¯ä»¥æ’å…¥BN å±‚
layers.Dense(10)
])
# åœ¨è®­ç»ƒé˜¶æ®µï¼Œéœ€è¦è®¾ç½®ç½‘ç»œçš„å‚æ•°training=True ä»¥åŒºåˆ†BN å±‚æ˜¯è®­ç»ƒè¿˜æ˜¯æµ‹è¯•æ¨¡å‹
with tf.GradientTape() as tape:
    # æ’å…¥é€šé“ç»´åº¦
	x = tf.expand_dims(x,axis=3)
	# å‰å‘è®¡ç®—ï¼Œè®¾ç½®è®¡ç®—æ¨¡å¼ï¼Œ[b, 784] => [b, 10]
	out = network(x, training=True)
# åœ¨æµ‹è¯•é˜¶æ®µï¼Œéœ€è¦è®¾ç½®training=Falseï¼Œé¿å…BN å±‚é‡‡ç”¨é”™è¯¯çš„è¡Œä¸º
for x,y in db_test: # éå†æµ‹è¯•é›†
	# æ’å…¥é€šé“ç»´åº¦
	x = tf.expand_dims(x,axis=3)
	# å‰å‘è®¡ç®—ï¼Œæµ‹è¯•æ¨¡å¼
	out = network(x, training=False)
```

#### 5.4.5 dropoutå±‚

dropoutåŒBNï¼Œéœ€è¦ä½¿ç”¨is_trainingè¿›è¡Œæ§åˆ¶

```python
tf.nn.dropout(x, rate=0.5)
layers.Dropout(rate=0.5)
```

#### 5.4.6 è¯å‘é‡å±‚(embeding)

```python
layers.Embedding(total_words, embedding_len,input_length=max_review_len)
x = tf.range(10) # ç”Ÿæˆ10 ä¸ªå•è¯çš„æ•°å­—ç¼–ç 
x = tf.random.shuffle(x) # æ‰“æ•£
# åˆ›å»ºå…±10 ä¸ªå•è¯ï¼Œæ¯ä¸ªå•è¯ç”¨é•¿åº¦ä¸º4 çš„å‘é‡è¡¨ç¤ºçš„å±‚
net = layers.Embedding(10, 4)
out = net(x) # è·å–è¯å‘é‡
# ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½è¯å‘é‡è¡¨
embed_glove = load_embed('glove.6B.50d.txt')
# ç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡è¡¨åˆå§‹åŒ–Embedding å±‚
net.set_weights([embed_glove])
# ç»è¿‡é¢„è®­ç»ƒçš„è¯å‘é‡æ¨¡å‹åˆå§‹åŒ–çš„Embedding å±‚å¯ä»¥è®¾ç½®ä¸ºä¸å‚ä¸è®­ç»ƒï¼šnet.trainable
# = Falseï¼Œé‚£ä¹ˆé¢„è®­ç»ƒçš„è¯å‘é‡å°±ç›´æ¥åº”ç”¨åˆ°æ­¤ç‰¹å®šä»»åŠ¡ä¸Šï¼›å¦‚æœå¸Œæœ›èƒ½å¤Ÿå­¦åˆ°åŒºåˆ«äºé¢„è®­
# ç»ƒè¯å‘é‡æ¨¡å‹ä¸åŒçš„è¡¨ç¤ºæ–¹æ³•ï¼Œé‚£ä¹ˆå¯ä»¥æŠŠEmbedding å±‚åŒ…å«è¿›åå‘ä¼ æ’­ç®—æ³•ä¸­å»ï¼Œåˆ©ç”¨
# æ¢¯åº¦ä¸‹é™æ¥å¾®è°ƒå•è¯è¡¨ç¤ºæ–¹æ³•ã€‚


# é¢„è®­ç»ƒè¯å‘é‡
print('Indexing word vectors.')
embeddings_index = {} # æå–å•è¯åŠå…¶å‘é‡ï¼Œä¿å­˜åœ¨å­—å…¸ä¸­
# è¯å‘é‡æ¨¡å‹æ–‡ä»¶å­˜å‚¨è·¯å¾„
GLOVE_DIR = r'C:\Users\z390\Downloads\glove6b50dtxt'
with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf-8') as f:
	for line in f:
		values = line.split()
		word = values[0]
		coefs = np.asarray(values[1:], dtype='float32')
		embeddings_index[word] = coefs
print('Found %s word vectors.' % len(embeddings_index))
# GloVe.6B ç‰ˆæœ¬å…±å­˜å‚¨äº†40 ä¸‡ä¸ªè¯æ±‡çš„å‘é‡è¡¨ã€‚æ ¹æ®è¯æ±‡çš„æ•°å­—ç¼–ç è¡¨ä¾æ¬¡ä»GloVe æ¨¡å‹ä¸­è·å–å…¶è¯å‘é‡ï¼Œå¹¶å†™å…¥å¯¹åº”ä½ç½®ã€‚
num_words = min(total_words, len(word_index))
embedding_matrix = np.zeros((num_words, embedding_len)) #è¯å‘é‡è¡¨
for word, i in word_index.items():
	if i >= MAX_NUM_WORDS:
		continue # è¿‡æ»¤æ‰å…¶ä»–è¯æ±‡
	embedding_vector = embeddings_index.get(word) # ä»GloVe æŸ¥è¯¢è¯å‘é‡
	if embedding_vector is not None:
		# words not found in embedding index will be all-zeros.
		embedding_matrix[i] = embedding_vector # å†™å…¥å¯¹åº”ä½ç½®
print(applied_vec_count, embedding_matrix.shape)
# åˆ›å»ºEmbedding å±‚
self.embedding = layers.Embedding(total_words, embedding_len,input_length=max_review_len,
	trainable=False)#ä¸å‚ä¸æ¢¯åº¦æ›´æ–°
self.embedding.build(input_shape=(None, max_review_len))
# åˆ©ç”¨GloVe æ¨¡å‹åˆå§‹åŒ–Embedding å±‚
self.embedding.set_weights([embedding_matrix])#åˆå§‹åŒ–
```

#### 5.4.7 å¾ªç¯ç¥ç»ç½‘ç»œå±‚

å¾ªç¯ç¥ç»ç½‘ç»œå±‚åŒ…å«RNNï¼ŒLSTMï¼ŒGRU

```python
layers.SimpleRNNCell(3) # åˆ›å»ºRNN Cellï¼Œå†…å­˜å‘é‡é•¿åº¦ä¸º3,éœ€è¦è‡ªå·±ç»´æŠ¤çŠ¶æ€å‘é‡
layers.SimpleRNNCell(units, dropout=0.5)  #ä½¿ç”¨dropout æŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆ
layers.SimpleRNN(64) # åˆ›å»ºçŠ¶æ€å‘é‡é•¿åº¦ä¸º64 çš„SimpleRNN å±‚
layers.SimpleRNN(64,return_sequences=True) # åˆ›å»ºRNN å±‚æ—¶ï¼Œè®¾ç½®è¿”å›æ‰€æœ‰æ—¶é—´æˆ³ä¸Šçš„è¾“å‡º
layers.LSTMCell(64) # åˆ›å»ºLSTM Cell
layers.LSTM(64, return_sequences=True)
layers.GRUCell(64) # æ–°å»ºGRU Cellï¼Œå‘é‡é•¿åº¦ä¸º64
layers.GRU(64, return_sequences=True)
net = keras.Sequential([ # æ„å»º2 å±‚RNN ç½‘ç»œ
    #é™¤æœ€æœ«å±‚å¤–ï¼Œéƒ½éœ€è¦è¿”å›æ‰€æœ‰æ—¶é—´æˆ³çš„è¾“å‡ºï¼Œç”¨ä½œä¸‹ä¸€å±‚çš„è¾“å…¥
	layers.SimpleRNN(64, return_sequences=True), 
	layers.SimpleRNN(64),
])
```

#### 5.4.8 onehot

```python
tf.one_hot(x,depth=10)
```

#### 5.4.9 æ–‡æœ¬å¤„ç†

```python
# æˆªæ–­å’Œå¡«å……å¥å­ï¼Œä½¿å¾—ç­‰é•¿ï¼Œæ­¤å¤„é•¿å¥å­ä¿ç•™å¥å­åé¢çš„éƒ¨åˆ†ï¼ŒçŸ­å¥å­åœ¨å‰é¢å¡«å……
x_train = keras.preprocessing.sequence.pad_sequences(x_train,maxlen=max_review_len)
x_test = keras.preprocessing.sequence.pad_sequences(x_test,maxlen=max_review_len)
```

## 6. è‡ªå®šä¹‰ç½‘ç»œ

### 6.1 è‡ªå®šä¹‰ç½‘ç»œå±‚

åœ¨åˆ›å»ºè‡ªå®šä¹‰ç½‘ç»œå±‚ç±»æ—¶ï¼Œéœ€è¦ç»§æ‰¿è‡ªlayers.Layer åŸºç±»  

è‡ªå®šä¹‰çš„ç½‘ç»œå±‚ï¼Œè‡³å°‘éœ€è¦å®ç°åˆå§‹åŒ–__init__æ–¹æ³•å’Œå‰å‘ä¼ æ’­é€»è¾‘call æ–¹æ³•

```python
# ä¸€ä¸ªæ²¡æœ‰åç½®å‘é‡çš„å…¨è¿æ¥å±‚,åŒæ—¶å›ºå®šæ¿€æ´»å‡½æ•°ä¸ºReLU å‡½æ•°
class MyDense(layers.Layer):
	# è‡ªå®šä¹‰ç½‘ç»œå±‚
	def __init__(self, inp_dim, outp_dim):
		super(MyDense, self).__init__()
		# åˆ›å»ºæƒå€¼å¼ é‡å¹¶æ·»åŠ åˆ°ç±»ç®¡ç†åˆ—è¡¨ä¸­ï¼Œè®¾ç½®ä¸ºéœ€è¦ä¼˜åŒ–
		self.kernel = self.add_variable('w', [inp_dim, outp_dim],trainable=True)
    def call(self, inputs, training=None):
        #training å‚æ•°ç”¨äºæŒ‡å®šæ¨¡å‹çš„çŠ¶æ€ï¼štraining ä¸ºTrue æ—¶æ‰§
		#è¡Œè®­ç»ƒæ¨¡å¼ï¼Œtraining ä¸ºFalse æ—¶æ‰§è¡Œæµ‹è¯•æ¨¡å¼ï¼Œé»˜è®¤å‚æ•°ä¸ºNoneï¼Œå³æµ‹è¯•æ¨¡å¼ã€‚ç”±äºå…¨è¿
		#æ¥å±‚çš„è®­ç»ƒæ¨¡å¼å’Œæµ‹è¯•æ¨¡å¼é€»è¾‘ä¸€è‡´ï¼Œæ­¤å¤„ä¸éœ€è¦é¢å¤–å¤„ç†ã€‚å¯¹äºéƒ¨ä»½æµ‹è¯•æ¨¡å¼å’Œè®­ç»ƒæ¨¡
		#å¼ä¸ä¸€è‡´çš„ç½‘ç»œå±‚ï¼Œéœ€è¦æ ¹æ®training å‚æ•°æ¥è®¾è®¡éœ€è¦æ‰§è¡Œçš„é€»è¾‘
		# å®ç°è‡ªå®šä¹‰ç±»çš„å‰å‘è®¡ç®—é€»è¾‘
		# X@W
		out = inputs @ self.kernel
		# æ‰§è¡Œæ¿€æ´»å‡½æ•°è¿ç®—
		out = tf.nn.relu(out)
		return out
```

### 6.3 ç½‘ç»œå®¹å™¨(Sequential)

å¯¹äºå¸¸è§çš„ç½‘ç»œï¼Œéœ€è¦æ‰‹åŠ¨è°ƒç”¨æ¯ä¸€å±‚çš„ç±»å®ä¾‹å®Œæˆå‰å‘ä¼ æ’­è¿ç®—ï¼Œå½“ç½‘ç»œå±‚æ•°å˜å¾—è¾ƒæ·±æ—¶ï¼Œè¿™ä¸€éƒ¨åˆ†ä»£ç æ˜¾å¾—éå¸¸è‡ƒè‚¿ã€‚å¯ä»¥é€šè¿‡Keras æä¾›çš„ç½‘ç»œå®¹å™¨Sequential å°†å¤šä¸ªç½‘ç»œå±‚å°è£…æˆä¸€ä¸ªå¤§ç½‘ç»œæ¨¡å‹ï¼Œåªéœ€è¦è°ƒç”¨ç½‘ç»œæ¨¡å‹çš„å®ä¾‹ä¸€æ¬¡å³å¯å®Œæˆæ•°æ®ä»ç¬¬ä¸€å±‚åˆ°æœ€æœ«å±‚çš„é¡ºåºä¼ æ’­è¿ç®—  

Sequential å®¹å™¨ä¹Ÿå¯ä»¥é€šè¿‡add()æ–¹æ³•ç»§ç»­è¿½åŠ æ–°çš„ç½‘ç»œå±‚ï¼Œå®ç°åŠ¨æ€åˆ›å»ºç½‘ç»œçš„åŠŸèƒ½

```python
# å¯¼å…¥Sequential å®¹å™¨
from tensorflow.keras import layers, Sequential
network = Sequential([ # å°è£…ä¸ºä¸€ä¸ªç½‘ç»œ
layers.Dense(3, activation=None), # å…¨è¿æ¥å±‚ï¼Œæ­¤å¤„ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
layers.ReLU(),#æ¿€æ´»å‡½æ•°å±‚
layers.Dense(2, activation=None), # å…¨è¿æ¥å±‚ï¼Œæ­¤å¤„ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
layers.ReLU() #æ¿€æ´»å‡½æ•°å±‚
])
x = tf.random.normal([4,3])
out = network(x) # è¾“å…¥ä»ç¬¬ä¸€å±‚å¼€å§‹ï¼Œé€å±‚ä¼ æ’­è‡³è¾“å‡ºå±‚ï¼Œå¹¶è¿”å›è¾“å‡ºå±‚çš„è¾“å‡º

layers_num = 2 # å †å 2 æ¬¡
network = Sequential([]) # å…ˆåˆ›å»ºç©ºçš„ç½‘ç»œå®¹å™¨
for _ in range(layers_num):
	network.add(layers.Dense(3)) # æ·»åŠ å…¨è¿æ¥å±‚
	network.add(layers.ReLU())# æ·»åŠ æ¿€æ´»å‡½æ•°å±‚
network.build(input_shape=(4, 4)) # åˆ›å»ºç½‘ç»œå‚æ•°
network.summary()
for p in network.trainable_variables:
	print(p.name, p.shape) # å‚æ•°åå’Œå½¢çŠ¶
```

### 6.3 è‡ªå®šä¹‰ç½‘ç»œ

åˆ›å»ºè‡ªå®šä¹‰çš„ç½‘ç»œç±»æ—¶ï¼Œéœ€è¦ç»§æ‰¿è‡ªkeras.Model åŸºç±»

Sequential å®¹å™¨æ–¹ä¾¿åœ°å°è£…æˆä¸€ä¸ªç½‘ç»œæ¨¡å‹,Sequential å®¹å™¨é€‚åˆäºæ•°æ®æŒ‰åºä»ç¬¬ä¸€å±‚ä¼ æ’­åˆ°ç¬¬äºŒå±‚ï¼Œå†ä»ç¬¬äºŒå±‚ä¼ æ’­åˆ°ç¬¬ä¸‰å±‚ï¼Œä»¥æ­¤è§„å¾‹ä¼ æ’­çš„ç½‘ç»œæ¨¡å‹ã€‚

å¯¹äºå¤æ‚çš„ç½‘ç»œç»“æ„ï¼Œä¾‹å¦‚ç¬¬ä¸‰å±‚çš„è¾“å…¥ä¸ä»…æ˜¯ç¬¬äºŒå±‚çš„è¾“å‡ºï¼Œè¿˜æœ‰ç¬¬ä¸€å±‚çš„è¾“å‡ºï¼Œæ­¤æ—¶ä½¿ç”¨è‡ªå®šä¹‰ç½‘ç»œæ›´åŠ çµæ´»ã€‚  

```python
class MyModel(keras.Model):
	# è‡ªå®šä¹‰ç½‘ç»œç±»ï¼Œç»§æ‰¿è‡ªModel åŸºç±»
	def __init__(self):
		super(MyModel, self).__init__()
		# å®Œæˆç½‘ç»œå†…éœ€è¦çš„ç½‘ç»œå±‚çš„åˆ›å»ºå·¥ä½œ
		self.fc1 = MyDense(28*28, 256)
		self.fc2 = MyDense(256, 128)
		self.fc3 = MyDense(128, 64)
		self.fc4 = MyDense(64, 32)
		self.fc5 = MyDense(32, 10)
	def call(self, inputs, training=None):
		# è‡ªå®šä¹‰å‰å‘è¿ç®—é€»è¾‘
		x = self.fc1(inputs)
		x = self.fc2(x)
		x = self.fc3(x)
		x = self.fc4(x)
		x = self.fc5(x)
		return x
model = Network() # åˆ›å»ºç½‘ç»œç±»å®ä¾‹
# é€šè¿‡build å‡½æ•°å®Œæˆå†…éƒ¨å¼ é‡çš„åˆ›å»ºï¼Œå…¶ä¸­4 ä¸ºä»»æ„è®¾ç½®çš„batch æ•°é‡ï¼Œ9 ä¸ºè¾“å…¥ç‰¹å¾é•¿åº¦
model.build(input_shape=(4, 9))
model.summary() # æ‰“å°ç½‘ç»œä¿¡æ¯
```

## 7. pipeline

### 7.1 æ•°æ®å¤„ç†

#### 7.1.1 Dataset

Datesetä¸»è¦åŠŸèƒ½æ˜¯å°†numpyè½¬æ¢ä¸ºDateset(from_tensor_slices)ã€éšæœºæ‰“æ•£ï¼ˆshuffleï¼‰ã€äº§ç”Ÿæ‰¹æ•°æ®ï¼ˆbatchï¼‰ã€é¢„å¤„ç†ï¼ˆmapï¼‰ã€å¾ªç¯äº§ç”Ÿæ•°æ®ï¼ˆrepeatï¼‰ã€‚è°ƒç”¨Datasetæä¾›çš„è¿™äº›å·¥å…·å‡½æ•°ä¼šè¿”å›æ–°çš„Dataset å¯¹è±¡ï¼Œå¯ä»¥é€šè¿‡db = db. step1(). step2(). step3 ()æ–¹å¼æŒ‰åºå®Œæˆæ‰€æœ‰çš„æ•°æ®å¤„ç†æ­¥éª¤

```python
train_db = tf.data.Dataset.from_tensor_slices((x, y)).map(preprocess).
		shuffle(10000).batch(128).repeat(30)
```

**è½¬æ¢æˆDataset å¯¹è±¡**:æ•°æ®åŠ è½½è¿›å…¥å†…å­˜åï¼Œéœ€è¦è½¬æ¢æˆDataset å¯¹è±¡ï¼Œæ‰èƒ½åˆ©ç”¨TensorFlow æä¾›çš„å„ç§ä¾¿æ·åŠŸèƒ½ã€‚é€šè¿‡Dataset.from_tensor_slices å¯ä»¥å°†è®­ç»ƒéƒ¨åˆ†çš„æ•°æ®å›¾ç‰‡x å’Œæ ‡ç­¾y éƒ½è½¬æ¢æˆDataset å¯¹è±¡ï¼š

```python
train_db = tf.data.Dataset.from_tensor_slices((x, y)) # æ„å»ºDataset å¯¹è±¡
```

**éšæœºæ‰“æ•£**ï¼šDataset.shuffle(buffer_size)

Dataset.shuffle(buffer_size)å·¥å…·å¯ä»¥è®¾ç½®Dataset å¯¹è±¡éšæœºæ‰“æ•£æ•°æ®ä¹‹é—´çš„é¡ºåºï¼Œé˜²æ­¢æ¯æ¬¡è®­ç»ƒæ—¶æ•°æ®æŒ‰å›ºå®šé¡ºåºäº§ç”Ÿï¼Œä»è€Œä½¿å¾—æ¨¡å‹å°è¯•â€œè®°å¿†â€ä½æ ‡ç­¾ä¿¡æ¯ï¼Œbuffer_size å‚æ•°æŒ‡å®šç¼“å†²æ± çš„å¤§å°ï¼Œä¸€èˆ¬è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒå¤§çš„å¸¸æ•°å³å¯

```python
train_db = train_db.shuffle(10000) # éšæœºæ‰“æ•£æ ·æœ¬ï¼Œä¸ä¼šæ‰“ä¹±æ ·æœ¬ä¸æ ‡ç­¾æ˜ å°„å…³ç³»
```

**æ‰¹è®­ç»ƒ**ï¼šä¸ºäº†åˆ©ç”¨æ˜¾å¡çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œä¸€èˆ¬åœ¨ç½‘ç»œçš„è®¡ç®—è¿‡ç¨‹ä¸­ä¼šåŒæ—¶è®¡ç®—å¤šä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬æŠŠè¿™ç§è®­ç»ƒæ–¹å¼å«åšæ‰¹è®­ç»ƒï¼Œå…¶ä¸­ä¸€ä¸ªæ‰¹ä¸­æ ·æœ¬çš„æ•°é‡å«åšBatch Sizeã€‚ä¸ºäº†ä¸€æ¬¡èƒ½å¤Ÿä»Dataset ä¸­äº§ç”ŸBatch Size æ•°é‡çš„æ ·æœ¬ï¼Œéœ€è¦è®¾ç½®Dataset ä¸ºæ‰¹è®­ç»ƒæ–¹å¼

```python
train_db = train_db.batch(128) # è®¾ç½®æ‰¹è®­ç»ƒï¼Œbatch size ä¸º128
æ„å»ºæ•°æ®é›†ï¼Œæ‰“æ•£ï¼Œæ‰¹é‡ï¼Œå¹¶ä¸¢æ‰æœ€åä¸€ä¸ªä¸å¤Ÿbatchsz çš„batch
db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))
db_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)
```

**é¢„å¤„ç†**:ä» keras.datasets ä¸­åŠ è½½çš„æ•°æ®é›†çš„æ ¼å¼å¤§éƒ¨åˆ†æƒ…å†µéƒ½ä¸èƒ½ç›´æ¥æ»¡è¶³æ¨¡å‹çš„è¾“å…¥è¦æ±‚ï¼Œå› æ­¤éœ€è¦æ ¹æ®ç”¨æˆ·çš„é€»è¾‘è‡ªè¡Œå®ç°é¢„å¤„ç†æ­¥éª¤ã€‚Dataset å¯¹è±¡é€šè¿‡æä¾›map(func)å·¥å…·å‡½æ•°ï¼Œå¯ä»¥éå¸¸æ–¹ä¾¿åœ°è°ƒç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„é¢„å¤„ç†é€»è¾‘ï¼Œå®ƒå®ç°åœ¨func å‡½æ•°é‡Œã€‚ä¾‹å¦‚ï¼Œä¸‹æ–¹ä»£ç è°ƒç”¨åä¸ºpreprocess çš„å‡½æ•°å®Œæˆæ¯ä¸ªæ ·æœ¬çš„é¢„å¤„ç†ï¼š

```python
# é¢„å¤„ç†å‡½æ•°å®ç°åœ¨preprocess å‡½æ•°ä¸­ï¼Œä¼ å…¥å‡½æ•°åå³å¯
train_db = train_db.map(preprocess)
def preprocess(x, y): # è‡ªå®šä¹‰çš„é¢„å¤„ç†å‡½æ•°
	# è°ƒç”¨æ­¤å‡½æ•°æ—¶ä¼šè‡ªåŠ¨ä¼ å…¥x,y å¯¹è±¡ï¼Œshape ä¸º[b, 28, 28], [b]
	# æ ‡å‡†åŒ–åˆ°0~1
	x = tf.cast(x, dtype=tf.float32) / 255.
	x = tf.reshape(x, [-1, 28*28]) # æ‰“å¹³
	y = tf.cast(y, dtype=tf.int32) # è½¬æˆæ•´å‹å¼ é‡
	y = tf.one_hot(y, depth=10) # one-hot ç¼–ç 
	# è¿”å›çš„x,y å°†æ›¿æ¢ä¼ å…¥çš„x,y å‚æ•°ï¼Œä»è€Œå®ç°æ•°æ®çš„é¢„å¤„ç†åŠŸèƒ½
	return x,y
```

**å¾ªç¯è®­ç»ƒ**ï¼š

```python
for epoch in range(20): # è®­ç»ƒEpoch æ•°
	for step, (x,y) in enumerate(train_db): # è¿­ä»£Step æ•°
	# training...

# ä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®Dataset å¯¹è±¡ï¼Œä½¿å¾—æ•°æ®é›†å¯¹è±¡å†…éƒ¨éå†å¤šæ¬¡æ‰ä¼šé€€å‡º
train_db = train_db.repeat(20) # æ•°æ®é›†è¿­ä»£20 éæ‰ç»ˆæ­¢
```

#### 7.1.2 ç»å…¸æ•°æ®é›†

åœ¨ TensorFlow ä¸­ï¼Œkeras.datasets æ¨¡å—æä¾›äº†å¸¸ç”¨ç»å…¸æ•°æ®é›†çš„è‡ªåŠ¨ä¸‹è½½ã€ç®¡ç†ã€åŠ è½½ä¸è½¬æ¢åŠŸèƒ½ï¼Œå¹¶ä¸”æä¾›äº†tf.data.Dataset æ•°æ®é›†å¯¹è±¡ï¼Œæ–¹ä¾¿å®ç°å¤šçº¿ç¨‹(Multi-threading)ã€é¢„å¤„ç†(Preprocessing)ã€éšæœºæ‰“æ•£(Shuffle)å’Œæ‰¹è®­ç»ƒ(Training on Batch)ç­‰å¸¸ç”¨æ•°æ®é›†çš„åŠŸèƒ½ã€‚

> å¸¸ç”¨çš„ç»å…¸æ•°æ®é›†
>
> * Boston Housingï¼Œæ³¢å£«é¡¿æˆ¿ä»·è¶‹åŠ¿æ•°æ®é›†ï¼Œç”¨äºå›å½’æ¨¡å‹è®­ç»ƒä¸æµ‹è¯•ã€‚
> * CIFAR10/100ï¼ŒçœŸå®å›¾ç‰‡æ•°æ®é›†ï¼Œç”¨äºå›¾ç‰‡åˆ†ç±»ä»»åŠ¡ã€‚
> * MNIST/Fashion_MNISTï¼Œæ‰‹å†™æ•°å­—å›¾ç‰‡æ•°æ®é›†ï¼Œç”¨äºå›¾ç‰‡åˆ†ç±»ä»»åŠ¡ã€‚
> * IMDBï¼Œæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡æ•°æ®é›†ï¼Œç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚

datasets.xxx.load_data()å‡½æ•°å³å¯å®ç°ç»å…¸æ•°æ®é›†çš„è‡ªåŠ¨åŠ è½½ï¼Œå…¶ä¸­xxx ä»£è¡¨å…·ä½“çš„æ•°æ®é›†åç§°ï¼Œå¦‚â€œCIFAR10â€ã€â€œMNISTâ€ã€‚TensorFlow ä¼šé»˜è®¤å°†æ•°æ®ç¼“å­˜åœ¨ç”¨æˆ·ç›®å½•ä¸‹çš„.keras/datasets æ–‡ä»¶å¤¹ï¼Œå¦‚æœå½“å‰
æ•°æ®é›†ä¸åœ¨ç¼“å­˜ä¸­ï¼Œåˆ™ä¼šè‡ªåŠ¨ä»ç½‘ç»œä¸‹è½½ã€è§£å‹å’ŒåŠ è½½æ•°æ®é›†ï¼›å¦‚æœå·²ç»åœ¨ç¼“å­˜ä¸­ï¼Œåˆ™è‡ªåŠ¨å®ŒæˆåŠ è½½ã€‚

å¯¹äºå›¾ç‰‡æ•°æ®é›†**MNISTã€CIFAR10 ç­‰ï¼Œä¼šè¿”å›2 ä¸ªtuple**ï¼Œç¬¬ä¸€ä¸ªtuple ä¿å­˜äº†ç”¨äºè®­ç»ƒçš„æ•°æ®x å’Œy è®­ç»ƒé›†å¯¹è±¡ï¼›ç¬¬2 ä¸ªtuple åˆ™ä¿å­˜äº†ç”¨äºæµ‹è¯•çš„æ•°æ®x_test å’Œy_test æµ‹è¯•é›†å¯¹è±¡ï¼Œæ‰€æœ‰çš„æ•°æ®éƒ½ç”¨Numpy æ•°ç»„å®¹å™¨ä¿å­˜

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets # å¯¼å…¥ç»å…¸æ•°æ®é›†åŠ è½½æ¨¡å—
# åŠ è½½MNIST æ•°æ®é›†
(x, y), (x_test, y_test) = datasets.mnist.load_data()
print('x:', x.shape, 'y:', y.shape, 'x test:', x_test.shape, 'y test:',y_test)
```

#### 7.1.3 è‡ªå®šä¹‰æ•°æ®é›†

**åˆ›å»ºç¼–ç è¡¨**

æ ·æœ¬çš„ç±»åˆ«ä¸€èˆ¬ä»¥å­—ç¬¦ä¸²ç±»å‹çš„ç±»åˆ«åæ ‡è®°ï¼Œä½†æ˜¯å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼Œé¦–å…ˆéœ€è¦å°†ç±»åˆ«åè¿›è¡Œ**æ•°å­—ç¼–ç **ï¼Œç„¶ååœ¨åˆé€‚çš„æ—¶å€™å†è½¬æ¢æˆOne-hot ç¼–ç æˆ–å…¶ä»–ç¼–ç æ ¼å¼ã€‚

ç±»åˆ«åä¸æ•°å­—çš„æ˜ å°„å…³ç³»ç§°ä¸ºç¼–ç è¡¨ï¼Œä¸€æ—¦åˆ›å»ºåï¼Œä¸€èˆ¬ä¸èƒ½å˜åŠ¨

ç¼–ç è¡¨çš„ç°æœ‰é”®å€¼å¯¹æ•°é‡ä½œä¸ºç±»åˆ«çš„æ ‡ç­¾æ˜ å°„æ•°å­—ï¼Œå¹¶ä¿å­˜è¿›name2label å­—å…¸å¯¹è±¡ã€‚

```python
def load_pokemon(root, mode='train'):
	# åˆ›å»ºæ•°å­—ç¼–ç è¡¨
	name2label = {} # ç¼–ç è¡¨å­—å…¸ï¼Œ"sq...":0
	# éå†æ ¹ç›®å½•ä¸‹çš„å­æ–‡ä»¶å¤¹ï¼Œå¹¶æ’åºï¼Œä¿è¯æ˜ å°„å…³ç³»å›ºå®š
	for name in sorted(os.listdir(os.path.join(root))):
		# è·³è¿‡éæ–‡ä»¶å¤¹å¯¹è±¡
		if not os.path.isdir(os.path.join(root, name)):
			continue
		# ç»™æ¯ä¸ªç±»åˆ«ç¼–ç ä¸€ä¸ªæ•°å­—
		name2label[name] = len(name2label.keys())
```

**åˆ›å»ºæ ·æœ¬å’Œæ ‡ç­¾è¡¨æ ¼**

ç¼–ç è¡¨ç¡®å®šåï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®å®é™…æ•°æ®çš„å­˜å‚¨æ–¹å¼è·å¾—æ¯ä¸ªæ ·æœ¬çš„å­˜å‚¨è·¯å¾„ä»¥åŠå®ƒçš„æ ‡ç­¾æ•°å­—ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸ºimages å’Œlabels ä¸¤ä¸ªList å¯¹è±¡ã€‚å…¶ä¸­images List å­˜å‚¨äº†æ¯ä¸ªæ ·æœ¬çš„è·¯å¾„å­—ç¬¦ä¸²ï¼Œlabels List å­˜å‚¨äº†æ ·æœ¬çš„ç±»åˆ«æ•°å­—ï¼Œä¸¤è€…é•¿åº¦ä¸€è‡´ï¼Œä¸”å¯¹åº”ä½ç½®çš„å…ƒç´ ç›¸äº’å…³è”ã€‚

æ¯è¡Œçš„ç¬¬ä¸€ä¸ªå…ƒç´ ä¿å­˜äº†å½“å‰æ ·æœ¬çš„å­˜å‚¨è·¯å¾„ï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¿å­˜äº†æ ·æœ¬çš„ç±»åˆ«æ•°å­—ã€‚

```python
def load_csv(root, filename, name2label):
	# ä»csv æ–‡ä»¶è¿”å›images,labels åˆ—è¡¨
	# root:æ•°æ®é›†æ ¹ç›®å½•ï¼Œfilename:csv æ–‡ä»¶åï¼Œ name2label:ç±»åˆ«åç¼–ç è¡¨
	if not os.path.exists(os.path.join(root, filename)):
        # å¦‚æœcsv æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
		images = []
		for name in name2label.keys(): # éå†æ‰€æœ‰å­ç›®å½•ï¼Œè·å¾—æ‰€æœ‰çš„å›¾ç‰‡
			# åªè€ƒè™‘åç¼€ä¸ºpng,jpg,jpeg çš„å›¾ç‰‡ï¼š'pokemon\\mewtwo\\00001.png
			images += glob.glob(os.path.join(root, name, '*.png'))
			images += glob.glob(os.path.join(root, name, '*.jpg'))
			images += glob.glob(os.path.join(root, name, '*.jpeg'))
        # æ‰“å°æ•°æ®é›†ä¿¡æ¯ï¼š1167, 'pokemon\\bulbasaur\\00000000.png'
		print(len(images), images)
		random.shuffle(images) # éšæœºæ‰“æ•£é¡ºåº
		# åˆ›å»ºcsv æ–‡ä»¶ï¼Œå¹¶å­˜å‚¨å›¾ç‰‡è·¯å¾„åŠå…¶label ä¿¡æ¯
		with open(os.path.join(root, filename), mode='w', newline='') as f:
			writer = csv.writer(f)
			for img in images: # 'pokemon\\bulbasaur\\00000000.png'
				name = img.split(os.sep)[-2]
				label = name2label[name]
				# 'pokemon\\bulbasaur\\00000000.png', 0
				writer.writerow([img, label])
			print('written into csv file:', filename)
	
    # æ­¤æ—¶å·²ç»æœ‰csv æ–‡ä»¶åœ¨æ–‡ä»¶ç³»ç»Ÿä¸Šï¼Œç›´æ¥è¯»å–
	images, labels = [], []
	with open(os.path.join(root, filename)) as f:
		reader = csv.reader(f)
		for row in reader:
			# 'pokemon\\bulbasaur\\00000000.png', 0
			img, label = row
			label = int(label)
			images.append(img)
			labels.append(label)
	# è¿”å›å›¾ç‰‡è·¯å¾„list å’Œæ ‡ç­¾list
	return images, labels
```

**æ•°æ®é›†åˆ’åˆ†**

æ•°æ®é›†çš„åˆ’åˆ†éœ€è¦æ ¹æ®å®é™…æƒ…å†µæ¥çµæ´»è°ƒæ•´åˆ’åˆ†æ¯”ç‡ã€‚å½“æ•°æ®é›†æ ·æœ¬æ•°è¾ƒå¤šæ—¶ï¼Œå¯ä»¥é€‰æ‹©80%-10%-10%çš„æ¯”ä¾‹åˆ†é…ç»™è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†;å¯¹äºå°å‹çš„æ•°æ®é›†ï¼Œå°½ç®¡æ ·æœ¬æ•°é‡è¾ƒå°ï¼Œä½†è¿˜æ˜¯éœ€è¦é€‚å½“å¢åŠ éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ¯”ä¾‹ï¼Œä»¥ä¿è¯è·å¾—å‡†ç¡®çš„æµ‹è¯•ç»“æœã€‚

```python
def load_pokemon(root, mode='train'):
	# è¯»å–Label ä¿¡æ¯
	# [file1,file2,], [3,1]
	images, labels = load_csv(root, 'images.csv', name2label)
	# æ•°æ®é›†åˆ’åˆ†
	if mode == 'train': # 60%
		images = images[:int(0.6 * len(images))]
		labels = labels[:int(0.6 * len(labels))]
	elif mode == 'val': # 20% = 60%->80%
		images = images[int(0.6 * len(images)):int(0.8 * len(images))]
		labels = labels[int(0.6 * len(labels)):int(0.8 * len(labels))]
	else: # 20% = 80%->100%
		images = images[int(0.8 * len(images)):]
		labels = labels[int(0.8 * len(labels)):]
	return images, labels, name2label
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ¯æ¬¡è¿è¡Œæ—¶çš„æ•°æ®é›†åˆ’åˆ†æ–¹æ¡ˆéœ€å›ºå®šï¼Œé˜²æ­¢ä½¿ç”¨æµ‹è¯•é›†çš„æ ·æœ¬è®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–æ€§èƒ½ä¸å‡†ç¡®

**åˆ›å»º Dataset å¯¹è±¡**

```python
# åŠ è½½pokemon æ•°æ®é›†ï¼ŒæŒ‡å®šåŠ è½½è®­ç»ƒé›†
# è¿”å›è®­ç»ƒé›†çš„æ ·æœ¬è·¯å¾„åˆ—è¡¨ï¼Œæ ‡ç­¾æ•°å­—åˆ—è¡¨å’Œç¼–ç è¡¨å­—å…¸
images, labels, table = load_pokemon('pokemon', 'train')
print('images:', len(images), images)
print('labels:', len(labels), labels)
print('table:', table)
# images: string path
# labels: number
db = tf.data.Dataset.from_tensor_slices((images, labels))
db = db.shuffle(1000).map(preprocess).batch(32)
```

**æ•°æ®é¢„å¤„ç†**

ä¸Šé¢æˆ‘ä»¬åœ¨æ„å»ºæ•°æ®é›†æ—¶é€šè¿‡è°ƒç”¨.map(preprocess)å‡½æ•°æ¥å®Œæˆæ•°æ®çš„é¢„å¤„ç†å·¥ä½œã€‚ç”±äºç›®å‰æˆ‘ä»¬çš„images åˆ—è¡¨åªæ˜¯ä¿å­˜äº†æ‰€æœ‰å›¾ç‰‡çš„è·¯å¾„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯å›¾ç‰‡çš„å†…å®¹å¼ é‡ï¼Œå› æ­¤éœ€è¦åœ¨é¢„å¤„ç†å‡½æ•°ä¸­å®Œæˆå›¾ç‰‡çš„è¯»å–ä»¥åŠå¼ é‡è½¬æ¢ç­‰å·¥ä½œã€‚

å¯¹äºé¢„å¤„ç†å‡½æ•°(x,y) = preprocess(x,y)ï¼Œå®ƒçš„ä¼ å…¥å‚æ•°éœ€è¦å’Œåˆ›å»ºDataset æ—¶ç»™çš„å‚æ•°çš„æ ¼å¼ä¿å­˜ä¸€è‡´ï¼Œè¿”å›å‚æ•°ä¹Ÿéœ€è¦å’Œä¼ å…¥å‚æ•°çš„æ ¼å¼ä¿å­˜ä¸€è‡´ã€‚

```python
def preprocess(x,y): # é¢„å¤„ç†å‡½æ•°
	# x: å›¾ç‰‡çš„è·¯å¾„ï¼Œyï¼šå›¾ç‰‡çš„æ•°å­—ç¼–ç 
	x = tf.io.read_file(x) # æ ¹æ®è·¯å¾„è¯»å–å›¾ç‰‡
	x = tf.image.decode_jpeg(x, channels=3) # å›¾ç‰‡è§£ç ï¼Œå¿½ç•¥é€æ˜é€šé“
	x = tf.image.resize(x, [244, 244]) # å›¾ç‰‡ç¼©æ”¾ä¸ºç•¥å¤§äº224 çš„244
	# æ•°æ®å¢å¼ºï¼Œè¿™é‡Œå¯ä»¥è‡ªç”±ç»„åˆå¢å¼ºæ‰‹æ®µ
	# x = tf.image.random_flip_up_down(x)
	x= tf.image.random_flip_left_right(x) # å·¦å³é•œåƒ
	x = tf.image.random_crop(x, [224, 224, 3]) # éšæœºè£å‰ªä¸º224
	# è½¬æ¢æˆå¼ é‡ï¼Œå¹¶å‹ç¼©åˆ°0~1 åŒºé—´
	# x: [0,255]=> 0~1
    x = tf.cast(x, dtype=tf.float32) / 255.
	# 0~1 => D(0,1)
	x = normalize(x) # æ ‡å‡†åŒ–
	y = tf.convert_to_tensor(y) # è½¬æ¢æˆå¼ é‡
	return x, y
```

å°†0~255 èŒƒå›´çš„åƒç´ å€¼ç¼©æ”¾åˆ°0~1 èŒƒå›´ï¼Œå¹¶é€šè¿‡æ ‡å‡†åŒ–å‡½æ•°normalize å®ç°æ•°æ®çš„æ ‡å‡†åŒ–è¿ç®—ï¼Œå°†åƒç´ æ˜ å°„ä¸º0 å‘¨å›´åˆ†å¸ƒï¼Œæœ‰åˆ©äºç½‘ç»œçš„ä¼˜åŒ–

æ ‡å‡†åŒ–åçš„æ•°æ®é€‚åˆç½‘ç»œçš„è®­ç»ƒåŠé¢„æµ‹ï¼Œä½†æ˜¯åœ¨è¿›è¡Œå¯è§†åŒ–æ—¶ï¼Œéœ€è¦å°†æ•°æ®æ˜ å°„å›0~1 çš„èŒƒå›´,å®ç°æ ‡å‡†åŒ–å’Œæ ‡å‡†åŒ–çš„é€†è¿‡ç¨‹å¦‚ä¸‹

```python
# è¿™é‡Œçš„mean å’Œstd æ ¹æ®çœŸå®çš„æ•°æ®è®¡ç®—è·å¾—ï¼Œæ¯”å¦‚ImageNet
img_mean = tf.constant([0.485, 0.456, 0.406])
img_std = tf.constant([0.229, 0.224, 0.225])
def normalize(x, mean=img_mean, std=img_std):
	# æ ‡å‡†åŒ–å‡½æ•°
	# x: [224, 224, 3]
	# mean: [224, 224, 3], std: [3]
	x = (x - mean)/std
	return x
def denormalize(x, mean=img_mean, std=img_std):
	# æ ‡å‡†åŒ–çš„é€†è¿‡ç¨‹å‡½æ•°
	x = x * std + mean
	return x
```

### 7.2 ç½‘ç»œæ„å»º

#### 7.2.1 å†…ç½®æ¨¡å‹ä½¿ç”¨

åœ¨keras.applications æ¨¡å—ä¸­å®ç°äº†å¸¸ç”¨çš„ç½‘ç»œæ¨¡å‹ï¼Œå¦‚VGG ç³»åˆ—ã€ResNet ç³»åˆ—ã€DenseNet ç³»åˆ—ã€MobileNet ç³»åˆ—ç­‰ç­‰ï¼Œåªéœ€è¦ä¸€è¡Œä»£ç å³å¯åˆ›å»ºè¿™äº›æ¨¡å‹ç½‘ç»œã€‚

```python
# åŠ è½½ImageNet é¢„è®­ç»ƒç½‘ç»œæ¨¡å‹ï¼Œå¹¶å»æ‰æœ€åä¸€å±‚
resnet = keras.applications.ResNet50(weights='imagenet',include_top=False)
resnet.summary()
# æµ‹è¯•ç½‘ç»œçš„è¾“å‡º
x = tf.random.normal([4,224,224,3])
out = resnet(x) # è·å¾—å­ç½‘ç»œçš„è¾“å‡º
out.shape
global_average_layer = layers.GlobalAveragePooling2D()
fc = layers.Dense(100)
mynet = Sequential([resnet, global_average_layer, fc])
mynet.summary()
#é€šè¿‡è®¾ç½®resnet.trainable = False å¯ä»¥é€‰æ‹©å†»ç»“ResNet éƒ¨åˆ†çš„ç½‘ç»œå‚æ•°ï¼Œåªè®­ç»ƒæ–°å»ºçš„
#ç½‘ç»œå±‚ï¼Œä»è€Œå¿«é€Ÿã€é«˜æ•ˆå®Œæˆç½‘ç»œæ¨¡å‹çš„è®­ç»ƒã€‚å½“ç„¶ä¹Ÿå¯ä»¥åœ¨è‡ªå®šä¹‰ä»»åŠ¡ä¸Šæ›´æ–°ç½‘ç»œçš„å…¨éƒ¨å‚æ•°ã€‚
```

#### 7.2.2 è¿ç§»å­¦ä¹ 

å¯¹äºå·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸€èˆ¬è®¤ä¸ºå®ƒèƒ½å¤Ÿé€å±‚æå–ç‰¹å¾ï¼Œè¶Šæœ«å±‚çš„ç½‘ç»œçš„æŠ½è±¡ç‰¹å¾æå–èƒ½åŠ›è¶Šå¼ºï¼Œè¾“å‡ºå±‚ä¸€èˆ¬ä½¿ç”¨ä¸ç±»åˆ«æ•°ç›¸åŒè¾“å‡ºèŠ‚ç‚¹çš„å…¨è¿æ¥å±‚ï¼Œä½œä¸ºåˆ†ç±»ç½‘ç»œçš„æ¦‚ç‡åˆ†å¸ƒé¢„æµ‹ã€‚å¯¹äºç›¸ä¼¼çš„ä»»åŠ¡A å’ŒBï¼Œå¦‚æœå®ƒä»¬çš„ç‰¹å¾æå–æ–¹æ³•æ˜¯ç›¸è¿‘çš„ï¼Œåˆ™ç½‘ç»œçš„å‰é¢æ•°å±‚å¯ä»¥é‡ç”¨ï¼Œç½‘ç»œåé¢çš„æ•°å±‚å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡è®¾å®šä»é›¶å¼€å§‹è®­ç»ƒ

```python
# åŠ è½½DenseNet ç½‘ç»œæ¨¡å‹ï¼Œå¹¶å»æ‰æœ€åä¸€å±‚å…¨è¿æ¥å±‚ï¼Œæœ€åä¸€ä¸ªæ± åŒ–å±‚è®¾ç½®ä¸ºmax pooling
# å¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„å‚æ•°åˆå§‹åŒ–
net = keras.applications.DenseNet121(weights='imagenet', include_top=False,pooling='max')
# è®¾è®¡ä¸ºä¸å‚ä¸ä¼˜åŒ–ï¼Œå³DenseNet è¿™éƒ¨åˆ†å‚æ•°å›ºå®šä¸åŠ¨
net.trainable = False
newnet = keras.Sequential([
	net, # å»æ‰æœ€åä¸€å±‚çš„DenseNet121
	layers.Dense(1024, activation='relu'), # è¿½åŠ å…¨è¿æ¥å±‚
	layers.BatchNormalization(), # è¿½åŠ BN å±‚
	layers.Dropout(rate=0.5), # è¿½åŠ Dropout å±‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
	layers.Dense(5) # æ ¹æ®å®å¯æ¢¦æ•°æ®çš„ä»»åŠ¡ï¼Œè®¾ç½®æœ€åä¸€å±‚è¾“å‡ºèŠ‚ç‚¹æ•°ä¸º5
])
newnet.build(input_shape=(4,224,224,3))
newnet.summary()
```

#### 7.2.3 è‡ªå®šä¹‰ç½‘ç»œä½¿ç”¨

å‚è€ƒ6.2

### 7.3 æ¨¡å‹è®­ç»ƒ

#### 7.3.1 å¸¸è§„è®­ç»ƒ

```python
optimizer = tf.keras.optimizers.RMSprop(0.001) # åˆ›å»ºä¼˜åŒ–å™¨ï¼ŒæŒ‡å®šå­¦ä¹ ç‡
for epoch in range(200): # 200 ä¸ªEpoch
	for step, (x,y) in enumerate(train_db): # éå†ä¸€æ¬¡è®­ç»ƒé›†
		# æ¢¯åº¦è®°å½•å™¨ï¼Œè®­ç»ƒæ—¶éœ€è¦ä½¿ç”¨å®ƒ
		with tf.GradientTape() as tape:
			out = model(x,training=True) # é€šè¿‡ç½‘ç»œè·å¾—è¾“å‡º
			loss = tf.reduce_mean(losses.MSE(y, out)) # è®¡ç®—MSE
			mae_loss = tf.reduce_mean(losses.MAE(y, out)) # è®¡ç®—MAE
		if step % 10 == 0: # é—´éš”æ€§åœ°æ‰“å°è®­ç»ƒè¯¯å·®
			print(epoch, step, float(loss))
		# è®¡ç®—æ¢¯åº¦ï¼Œå¹¶æ›´æ–°
		grads = tape.gradient(loss, model.trainable_variables)
		optimizer.apply_gradients(zip(grads, model.trainable_variables))
    # åœ¨æµ‹è¯•é˜¶æ®µï¼Œéœ€è¦è®¾ç½®training=Falseï¼Œé¿å…BN å±‚é‡‡ç”¨é”™è¯¯çš„è¡Œä¸º
	for x,y in test_db: # éå†æµ‹è¯•é›†
		# å‰å‘è®¡ç®—ï¼Œæµ‹è¯•æ¨¡å¼
		out = network(x, training=False)
```

#### 7.3.2 è£…é…è®­ç»ƒ

è£…é…ï¼šæŒ‡å®šç½‘ç»œä½¿ç”¨çš„ä¼˜åŒ–å™¨å¯¹è±¡ã€æŸå¤±å‡½æ•°ç±»å‹ï¼Œè¯„ä»·æŒ‡æ ‡ç­‰

```python
network.compile(optimizer=optimizers.Adam(lr=0.01),
loss=losses.CategoricalCrossentropy(from_logits=True),
metrics=['accuracy'] # è®¾ç½®æµ‹é‡æŒ‡æ ‡ä¸ºå‡†ç¡®ç‡
)
```

è®­ç»ƒ:é€å…¥å¾…è®­ç»ƒçš„æ•°æ®é›†å’ŒéªŒè¯ç”¨çš„æ•°æ®é›†ï¼Œè¿™ä¸€æ­¥ç§°ä¸ºæ¨¡å‹è®­ç»ƒ

```python
# æŒ‡å®šè®­ç»ƒé›†ä¸ºtrain_dbï¼ŒéªŒè¯é›†ä¸ºval_db,è®­ç»ƒ5 ä¸ªepochsï¼Œæ¯2 ä¸ªepoch éªŒè¯ä¸€æ¬¡
# è¿”å›è®­ç»ƒè½¨è¿¹ä¿¡æ¯ä¿å­˜åœ¨history å¯¹è±¡ä¸­
history = network.fit(train_db, epochs=5, validation_data=val_db,validation_freq=2,callbacks=[early_stopping])
# train_db ä¸ºtf.data.Dataset å¯¹è±¡ï¼Œä¹Ÿå¯ä»¥ä¼ å…¥Numpy Array ç±»å‹çš„æ•°æ®ï¼›epochs å‚æ•°æŒ‡
# å®šè®­ç»ƒè¿­ä»£çš„Epoch æ•°é‡ï¼›validation_data å‚æ•°æŒ‡å®šç”¨äºéªŒè¯(æµ‹è¯•)çš„æ•°æ®é›†å’ŒéªŒè¯çš„é¢‘ç‡validation_freqã€‚
# fit å‡½æ•°ä¼šè¿”å›è®­ç»ƒè¿‡ç¨‹çš„æ•°æ®è®°å½•historyï¼Œå…¶ä¸­history.history ä¸ºå­—å…¸å¯¹è±¡ï¼ŒåŒ…å«äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„lossã€
# æµ‹é‡æŒ‡æ ‡ç­‰è®°å½•é¡¹ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥æŸ¥çœ‹è¿™äº›è®­ç»ƒæ•°æ®ï¼Œ
# åˆ›å»ºEarly Stopping ç±»ï¼Œè¿ç»­3 æ¬¡ä¸ä¸Šå‡åˆ™ç»ˆæ­¢è®­ç»ƒ
early_stopping = EarlyStopping(monitor='val_accuracy',
			min_delta=0.001,patience=3)
```

æµ‹è¯•ã€é¢„æµ‹ï¼šå‰å‘è®¡ç®—

```python
# åŠ è½½ä¸€ä¸ªbatch çš„æµ‹è¯•æ•°æ®
x,y = next(iter(db_test))
print('predict x:', x.shape) # æ‰“å°å½“å‰batch çš„å½¢çŠ¶
# predictè®¡ç®—ä¸€æ¬¡
out = network.predict(x) # æ¨¡å‹é¢„æµ‹ï¼Œé¢„æµ‹ç»“æœä¿å­˜åœ¨out ä¸­
# evaluateè®¡ç®—æ•´ä¸ªæ•°æ®é›†
network.evaluate(db_test) # æ¨¡å‹æµ‹è¯•ï¼Œæµ‹è¯•åœ¨db_test ä¸Šçš„æ€§èƒ½è¡¨ç°
```

### 7.4 æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

**å¼ é‡æ–¹å¼**ï¼šåªä¿å­˜æ¨¡å‹å‚æ•°ï¼Œä¸ä¿å­˜æ¨¡å‹ç»“æ„ï¼Œéœ€è¦ä½¿ç”¨ç›¸åŒçš„ç½‘ç»œç»“æ„æ‰èƒ½å¤Ÿæ­£ç¡®æ¢å¤ç½‘ç»œçŠ¶æ€ï¼Œå› æ­¤ä¸€èˆ¬
åœ¨æ‹¥æœ‰ç½‘ç»œæºæ–‡ä»¶çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚  

```python
network.save_weights('weights.ckpt')
network.load_weights('weights.ckpt')
```

**ç½‘ç»œæ–¹å¼**:ä¿å­˜æ¨¡å‹ç»“æ„å’Œæ¨¡å‹å‚æ•°ï¼Œå¯ç›´æ¥æ¢å¤å‡ºç½‘ç»œæ¨¡å‹

```python
network.save('model.h5')
print('saved total model.')
del network
# ä»æ–‡ä»¶æ¢å¤ç½‘ç»œç»“æ„ä¸ç½‘ç»œå‚æ•°
network = keras.models.load_model('model.h5')
```

**SavedModel æ–¹å¼**ï¼šå°†æ¨¡å‹ä¿å­˜ä¸ºpbï¼Œåœ¨ç§»åŠ¨ç«¯è¿è¡Œ

```python
tf.saved_model.save(network, 'model-savedmodel')
# ä»æ–‡ä»¶æ¢å¤ç½‘ç»œç»“æ„ä¸ç½‘ç»œå‚æ•°
network = tf.saved_model.load('model-savedmodel')
```

### 7.5 å¯è§†åŒ–

å¯è§†åŒ–ï¼ˆtensorboardï¼‰

```python
# åˆ›å»ºç›‘æ§ç±»ï¼Œç›‘æ§æ•°æ®å°†å†™å…¥log_dir ç›®å½•
summary_writer = tf.summary.create_file_writer(log_dir)
with summary_writer.as_default(): # å†™å…¥ç¯å¢ƒ
	# å½“å‰æ—¶é—´æˆ³step ä¸Šçš„æ•°æ®ä¸ºlossï¼Œå†™å…¥åˆ°åä¸ºtrain-loss æ•°æ®åº“ä¸­
	tf.summary.scalar('train-loss', float(loss), step=step)
    # å¯è§†åŒ–æµ‹è¯•ç”¨çš„å›¾ç‰‡ï¼Œè®¾ç½®æœ€å¤šå¯è§†åŒ–9 å¼ å›¾ç‰‡
	tf.summary.image("val-onebyone-images:", val_images,max_outputs=9, step=step)
    # å¯è§†åŒ–çœŸå®æ ‡ç­¾çš„ç›´æ–¹å›¾åˆ†å¸ƒ
	tf.summary.histogram('y-hist',y, step=step)
	# æŸ¥çœ‹æ–‡æœ¬ä¿¡æ¯
	tf.summary.text('loss-text',str(float(loss)))
```

## 8. æ¨¡å‹è°ƒä¼˜

### 8.1 è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ

#### 8.1.1 è¿‡æ‹Ÿåˆ

åœ¨è®­ç»ƒé›†ä¸Šé¢è¡¨ç°è¾ƒå¥½ï¼Œä½†æ˜¯åœ¨æœªè§çš„æ ·æœ¬ä¸Šè¡¨ç°ä¸ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹æ³›åŒ–èƒ½åŠ›åå¼±

ç¼“è§£è¿‡æ‹Ÿåˆæ–¹æ³•ï¼š

é™ä½ç½‘ç»œçš„å±‚æ•°ã€é™ä½ç½‘ç»œçš„å‚æ•°é‡ã€æ·»åŠ æ­£åˆ™åŒ–æ‰‹æ®µã€æ·»åŠ å‡è®¾ç©ºé—´çš„çº¦æŸ

å¯¹äºç¥ç»ç½‘ç»œï¼Œå³ä½¿ç½‘ç»œç»“æ„è¶…å‚æ•°ä¿æŒä¸å˜(å³ç½‘ç»œæœ€å¤§å®¹é‡å›ºå®š)ï¼Œæ¨¡å‹ä¾ç„¶å¯èƒ½ä¼šå‡ºç°è¿‡æ‹Ÿåˆçš„ç°è±¡ï¼Œè¿™æ˜¯å› ä¸ºç¥ç»ç½‘ç»œçš„æœ‰æ•ˆå®¹é‡å’Œç½‘ç»œå‚æ•°çš„çŠ¶æ€æ¯æ¯ç›¸å…³ï¼Œç¥ç»ç½‘ç»œçš„æœ‰æ•ˆå®¹é‡å¯ä»¥å¾ˆå¤§ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç¨€ç–åŒ–å‚æ•°ã€æ·»åŠ æ­£åˆ™åŒ–ç­‰æ‰‹æ®µé™ä½æœ‰æ•ˆå®¹é‡

1. æå‰åœæ­¢

ä¸€èˆ¬æŠŠå¯¹è®­ç»ƒé›†ä¸­çš„ä¸€ä¸ªBatch è¿ç®—æ›´æ–°ä¸€æ¬¡å«åšä¸€ä¸ªStepï¼Œå¯¹è®­ç»ƒé›†çš„æ‰€æœ‰æ ·æœ¬å¾ªç¯è¿­ä»£ä¸€æ¬¡å«åšä¸€ä¸ªEpochã€‚éªŒè¯é›†å¯ä»¥åœ¨æ•°æ¬¡Step æˆ–æ•°æ¬¡Epoch åä½¿ç”¨ï¼Œè®¡ç®—æ¨¡å‹çš„éªŒæ€§èƒ½ã€‚å½“å‘ç°éªŒè¯å‡†ç¡®ç‡è¿ç»­ğ‘›ä¸ªEpoch æ²¡æœ‰ä¸‹é™æ—¶ï¼Œå¯ä»¥é¢„æµ‹å¯èƒ½å·²ç»è¾¾åˆ°äº†æœ€é€‚åˆçš„Epoch é™„è¿‘ï¼Œä»è€Œæå‰ç»ˆæ­¢è®­ç»ƒ

2. æ¨¡å‹è®¾è®¡

å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼Œç½‘ç»œçš„å±‚æ•°å’Œå‚æ•°é‡æ˜¯ç½‘ç»œå®¹é‡å¾ˆé‡è¦çš„å‚è€ƒæŒ‡æ ‡ï¼Œé€šè¿‡å‡å°‘ç½‘ç»œçš„å±‚æ•°ï¼Œå¹¶å‡å°‘æ¯å±‚ä¸­ç½‘ç»œå‚æ•°é‡çš„è§„æ¨¡ï¼Œå¯ä»¥æœ‰æ•ˆé™ä½ç½‘ç»œçš„å®¹é‡ã€‚åä¹‹ï¼Œå¦‚æœå‘ç°æ¨¡å‹æ¬ æ‹Ÿåˆï¼Œéœ€è¦å¢å¤§ç½‘ç»œçš„å®¹é‡ï¼Œå¯ä»¥é€šè¿‡å¢åŠ å±‚æ•°ï¼Œå¢å¤§æ¯å±‚çš„å‚æ•°é‡ç­‰æ–¹å¼å®ç°ã€‚

3. æ­£åˆ™åŒ–

é€šè¿‡è®¾è®¡ä¸åŒå±‚æ•°ã€å¤§å°çš„ç½‘ç»œæ¨¡å‹å¯ä»¥ä¸ºä¼˜åŒ–ç®—æ³•æä¾›åˆå§‹çš„å‡½æ•°å‡è®¾ç©ºé—´ï¼Œä½†æ˜¯æ¨¡å‹çš„å®é™…å®¹é‡å¯ä»¥éšç€ç½‘ç»œå‚æ•°çš„ä¼˜åŒ–æ›´æ–°è€Œäº§ç”Ÿå˜åŒ–ã€‚é€šè¿‡é™åˆ¶ç½‘ç»œå‚æ•°çš„ç¨€ç–æ€§ï¼Œå¯ä»¥æ¥çº¦æŸç½‘ç»œçš„å®é™…å®¹é‡ã€‚

è¿™ç§çº¦æŸä¸€èˆ¬é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸Šæ·»åŠ é¢å¤–çš„å‚æ•°ç¨€ç–æ€§æƒ©ç½šé¡¹å®ç°ï¼Œä¸€èˆ¬åœ°ï¼Œå‚æ•°ğœƒçš„ç¨€ç–æ€§çº¦æŸé€šè¿‡çº¦æŸå‚æ•°
ğœƒçš„ğ¿èŒƒæ•°å®ç°ï¼Œè¿‡å¤§çš„ğœ†å‚æ•°æœ‰å¯èƒ½å¯¼è‡´ç½‘ç»œä¸æ”¶æ•›ï¼Œéœ€è¦æ ¹æ®å®é™…ä»»åŠ¡è°ƒèŠ‚

â€‹			ğ‘šğ‘–ğ‘› â„’(ğ‘“ğœƒ( ), ğ‘¦) + ğœ† âˆ™ ğ›º(ğœƒ), ( , ğ‘¦) âˆˆ ğ”»train

L0 æ­£åˆ™åŒ–ï¼šéé›¶å…ƒç´ çš„ä¸ªæ•°ï¼Œé€šè¿‡çº¦æŸL0èŒƒæ•°çš„å¤§å°å¯ä»¥è¿«ä½¿ç½‘ç»œä¸­çš„è¿æ¥æƒå€¼å¤§éƒ¨åˆ†ä¸º0ï¼Œä»è€Œé™ä½ç½‘ç»œçš„å®é™…å‚æ•°é‡å’Œç½‘ç»œå®¹é‡ï¼ŒL0 èŒƒæ•°å¹¶ä¸å¯å¯¼ï¼Œä¸èƒ½åˆ©ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œåœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨çš„å¹¶ä¸å¤š

L0 æ­£åˆ™åŒ–ï¼šç»å¯¹å€¼ä¹‹å’Œ

L2 æ­£åˆ™åŒ–ï¼šå¹³æ–¹å’Œ

4. Dropout

Dropout é€šè¿‡éšæœºæ–­å¼€ç¥ç»ç½‘ç»œçš„è¿æ¥ï¼Œå‡å°‘æ¯æ¬¡è®­ç»ƒæ—¶å®é™…å‚ä¸è®¡ç®—çš„æ¨¡å‹çš„å‚æ•°é‡ï¼›ä½†æ˜¯åœ¨æµ‹è¯•æ—¶ï¼ŒDropout ä¼šæ¢å¤æ‰€æœ‰çš„è¿æ¥ï¼Œä¿è¯æ¨¡å‹æµ‹è¯•æ—¶è·å¾—æœ€å¥½çš„æ€§èƒ½ã€‚

5. æ•°æ®å¢å¼º

TensorFlow ä¸­æä¾›äº†å¸¸ç”¨å›¾ç‰‡çš„å¤„ç†å‡½æ•°ï¼Œä½äºtf.image å­æ¨¡å—ä¸­ã€‚

```python
tf.image.resize(x, [244, 244])
# å›¾ç‰‡é€†æ—¶é’ˆæ—‹è½¬180 åº¦
x = tf.image.rot90(x,2)
# éšæœºæ°´å¹³ç¿»è½¬
x = tf.image.random_flip_left_right(x)
# éšæœºç«–ç›´ç¿»è½¬
x = tf.image.random_flip_up_down(x)
# å›¾ç‰‡å…ˆç¼©æ”¾åˆ°ç¨å¤§å°ºå¯¸
x = tf.image.resize(x, [244, 244])
# å†éšæœºè£å‰ªåˆ°åˆé€‚å°ºå¯¸
x = tf.image.random_crop(x, [224,224,3])
```

6. ç”Ÿæˆæ•°æ®

#### 8.1.2 æ¬ æ‹Ÿåˆ

è®­ç»ƒé›†ä¸Šè¡¨ç°ä¸ä½³ï¼ŒåŒæ—¶åœ¨æœªè§çš„æ ·æœ¬ä¸Šè¡¨ç°ä¹Ÿä¸ä½³

æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¯¯å·®ä¸€ç›´ç»´æŒè¾ƒé«˜çš„çŠ¶æ€ï¼Œå¾ˆéš¾ä¼˜åŒ–å‡å°‘ï¼ŒåŒæ—¶åœ¨æµ‹è¯•é›†ä¸Šä¹Ÿè¡¨ç°ä¸ä½³æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘æ˜¯å¦å‡ºç°äº†æ¬ æ‹Ÿåˆçš„ç°è±¡ã€‚

å¯ä»¥é€šè¿‡å¢åŠ ç¥ç»ç½‘ç»œçš„å±‚æ•°ã€å¢å¤§ä¸­é—´ç»´åº¦çš„å¤§å°ç­‰æ‰‹æ®µï¼Œæ¯”è¾ƒå¥½çš„è§£å†³æ¬ æ‹Ÿåˆçš„é—®é¢˜

åŠ æ·±ç½‘ç»œçš„å±‚æ•°ã€å¢åŠ ç½‘ç»œçš„å‚æ•°é‡ï¼Œå°è¯•æ›´å¤æ‚çš„ç½‘ç»œç»“æ„ã€‚

### 8.2 æ¢¯åº¦å¼¥æ•£å’Œæ¢¯åº¦æ¶ˆå¤±

#### 8.2.1 æ¢¯åº¦å¼¥æ•£

æ¢¯åº¦å¼¥æ•£(Gradient Vanishing)ï¼šæ¢¯åº¦å€¼æ¥è¿‘äº0 çš„ç°è±¡å«åšæ¢¯åº¦å¼¥æ•£

å¯¹äºæ¢¯åº¦å¼¥æ•£ç°è±¡ï¼Œå¯ä»¥é€šè¿‡å¢å¤§å­¦ä¹ ç‡ã€å‡å°‘ç½‘ç»œæ·±åº¦ã€æ·»åŠ Skip Connection ç­‰ä¸€ç³»åˆ—çš„æªæ–½æŠ‘åˆ¶ã€‚

#### 8.2.2 æ¢¯åº¦æ¶ˆå¤±

æ¢¯åº¦çˆ†ç‚¸(Gradient Exploding)ï¼šæ¢¯åº¦å€¼è¿œå¤§äº1 çš„ç°è±¡å«åšæ¢¯åº¦çˆ†ç‚¸(Gradient Exploding)

æ¢¯åº¦çˆ†ç‚¸å¯ä»¥é€šè¿‡æ¢¯åº¦è£å‰ª(Gradient Clipping)çš„æ–¹å¼åœ¨ä¸€å®šç¨‹åº¦ä¸Šçš„è§£å†³ã€‚æ¢¯åº¦è£å‰ªä¸å¼ é‡é™å¹…éå¸¸ç±»ä¼¼ï¼Œä¹Ÿæ˜¯é€šè¿‡å°†æ¢¯åº¦å¼ é‡çš„æ•°å€¼æˆ–è€…èŒƒæ•°é™åˆ¶åœ¨æŸä¸ªè¾ƒå°çš„åŒºé—´å†…ï¼Œä»è€Œå°†è¿œå¤§äº1 çš„æ¢¯åº¦å€¼å‡å°‘ï¼Œé¿å…å‡ºç°æ¢¯åº¦çˆ†ç‚¸

tf.clip_by_value(a,0.4,0.6) # æ¢¯åº¦å€¼è£å‰ª

tf.clip_by_norm(a,5) æŒ‰èŒƒæ•°æ–¹å¼è£å‰ª

tf.clip_by_global_norm  ç¼©æ”¾æ•´ä½“ç½‘ç»œæ¢¯åº¦ğ‘¾çš„èŒƒæ•°

```python
with tf.GradientTape() as tape:
	logits = model(x) # å‰å‘ä¼ æ’­
	loss = criteon(y, logits) # è¯¯å·®è®¡ç®—
# è®¡ç®—æ¢¯åº¦å€¼
grads = tape.gradient(loss, model.trainable_variables)
grads, _ = tf.clip_by_global_norm(grads, 25) # å…¨å±€æ¢¯åº¦è£å‰ª
# åˆ©ç”¨è£å‰ªåçš„æ¢¯åº¦å¼ é‡æ›´æ–°å‚æ•°
optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

## 9 ç½‘ç»œå®ç°å®ä¾‹

### 9.1 VGG13

```python
conv_layers = [ # å…ˆåˆ›å»ºåŒ…å«å¤šç½‘ç»œå±‚çš„åˆ—è¡¨
# Conv-Conv-Pooling å•å…ƒ1
# 64 ä¸ª3x3 å·ç§¯æ ¸, è¾“å…¥è¾“å‡ºåŒå¤§å°
layers.Conv2D(64, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.Conv2D(64, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
# é«˜å®½å‡åŠ
layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),
# Conv-Conv-Pooling å•å…ƒ2,è¾“å‡ºé€šé“æå‡è‡³128ï¼Œé«˜å®½å¤§å°å‡åŠ
layers.Conv2D(128, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.Conv2D(128, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),
# Conv-Conv-Pooling å•å…ƒ3,è¾“å‡ºé€šé“æå‡è‡³256ï¼Œé«˜å®½å¤§å°å‡åŠ
layers.Conv2D(256, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.Conv2D(256, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),
# Conv-Conv-Pooling å•å…ƒ4,è¾“å‡ºé€šé“æå‡è‡³512ï¼Œé«˜å®½å¤§å°å‡åŠ
layers.Conv2D(512, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.Conv2D(512, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),
# Conv-Conv-Pooling å•å…ƒ5,è¾“å‡ºé€šé“æå‡è‡³512ï¼Œé«˜å®½å¤§å°å‡åŠ
layers.Conv2D(512, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.Conv2D(512, kernel_size=[3, 3], padding="same", activation=tf.nn.relu),
layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same')
]
# åˆ©ç”¨å‰é¢åˆ›å»ºçš„å±‚åˆ—è¡¨æ„å»ºç½‘ç»œå®¹å™¨
conv_net = Sequential(conv_layers)
# åˆ›å»º3 å±‚å…¨è¿æ¥å±‚å­ç½‘ç»œ
fc_net = Sequential([
layers.Dense(256, activation=tf.nn.relu),
layers.Dense(128, activation=tf.nn.relu),
layers.Dense(10, activation=None),
])
# build2 ä¸ªå­ç½‘ç»œï¼Œå¹¶æ‰“å°ç½‘ç»œå‚æ•°ä¿¡æ¯
conv_net.build(input_shape=[4, 32, 32, 3])
fc_net.build(input_shape=[4, 512])
conv_net.summary()
fc_net.summary()
# åˆ—è¡¨åˆå¹¶ï¼Œåˆå¹¶2 ä¸ªå­ç½‘ç»œçš„å‚æ•°
variables = conv_net.trainable_variables + fc_net.trainable_variables
# å¯¹æ‰€æœ‰å‚æ•°æ±‚æ¢¯åº¦
grads = tape.gradient(loss, variables)
# è‡ªåŠ¨æ›´æ–°
optimizer.apply_gradients(zip(grads, variables))
```

### 9.2 resnet

```python
class BasicBlock(layers.Layer):
	# æ®‹å·®æ¨¡å—ç±»
	def __init__(self, filter_num, stride=1):
        super(BasicBlock, self).__init__()
		# f(x)åŒ…å«äº†2 ä¸ªæ™®é€šå·ç§¯å±‚ï¼Œåˆ›å»ºå·ç§¯å±‚1
		self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same')
		self.bn1 = layers.BatchNormalization()
		self.relu = layers.Activation('relu')
		# åˆ›å»ºå·ç§¯å±‚2
		self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')
		self.bn2 = layers.BatchNormalization()
        #å½“â„±(ğ’™)çš„å½¢çŠ¶ä¸ğ’™ä¸åŒæ—¶ï¼Œæ— æ³•ç›´æ¥ç›¸åŠ ï¼Œæˆ‘ä»¬éœ€è¦æ–°å»ºidentity(ğ’™)å·ç§¯å±‚ï¼Œæ¥å®Œæˆğ’™çš„å½¢çŠ¶è½¬æ¢
    	if stride != 1: # æ’å…¥identity å±‚
			self.downsample = Sequential()
			self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))
		else: # å¦åˆ™ï¼Œç›´æ¥è¿æ¥
			self.downsample = lambda x:x
    def call(self, inputs, training=None):
		# å‰å‘ä¼ æ’­å‡½æ•°
		out = self.conv1(inputs) # é€šè¿‡ç¬¬ä¸€ä¸ªå·ç§¯å±‚
		out = self.bn1(out)
		out = self.relu(out)
		out = self.conv2(out) # é€šè¿‡ç¬¬äºŒä¸ªå·ç§¯å±‚
		out = self.bn2(out)
		# è¾“å…¥é€šè¿‡identity()è½¬æ¢
		identity = self.downsample(inputs)
		# f(x)+x è¿ç®—
		output = layers.add([out, identity])
		# å†é€šè¿‡æ¿€æ´»å‡½æ•°å¹¶è¿”å›
		output = tf.nn.relu(output)
		return output
```

```python
def build_resblock(self, filter_num, blocks, stride=1):
	# è¾…åŠ©å‡½æ•°ï¼Œå †å filter_num ä¸ªBasicBlock
	res_blocks = Sequential()
	# åªæœ‰ç¬¬ä¸€ä¸ªBasicBlock çš„æ­¥é•¿å¯èƒ½ä¸ä¸º1ï¼Œå®ç°ä¸‹é‡‡æ ·
	res_blocks.add(BasicBlock(filter_num, stride))
	for _ in range(1, blocks):#å…¶ä»–BasicBlock æ­¥é•¿éƒ½ä¸º1
		res_blocks.add(BasicBlock(filter_num, stride=1))
	return res_blocks
class ResNet(keras.Model):
	# é€šç”¨çš„ResNet å®ç°ç±»
	def __init__(self, layer_dims, num_classes=10): # [2, 2, 2, 2]
		super(ResNet, self).__init__()
		# æ ¹ç½‘ç»œï¼Œé¢„å¤„ç†
		self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),
			layers.BatchNormalization(),
			layers.Activation('relu'),
			layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1),padding='same')
			])
		# å †å 4 ä¸ªBlockï¼Œæ¯ä¸ªblock åŒ…å«äº†å¤šä¸ªBasicBlock,è®¾ç½®æ­¥é•¿ä¸ä¸€æ ·
		self.layer1 = self.build_resblock(64, layer_dims[0])
		self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)
		self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)
		self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)
		# é€šè¿‡Pooling å±‚å°†é«˜å®½é™ä½ä¸º1x1
		self.avgpool = layers.GlobalAveragePooling2D()
		# æœ€åè¿æ¥ä¸€ä¸ªå…¨è¿æ¥å±‚åˆ†ç±»
		self.fc = layers.Dense(num_classes)
	def call(self, inputs, training=None):
		# å‰å‘è®¡ç®—å‡½æ•°ï¼šé€šè¿‡æ ¹ç½‘ç»œ
		x = self.stem(inputs)
		# ä¸€æ¬¡é€šè¿‡4 ä¸ªæ¨¡å—
		x = self.layer1(x)
		x = self.layer2(x)
		x = self.layer3(x)
		x = self.layer4(x)
		# é€šè¿‡æ± åŒ–å±‚
		x = self.avgpool(x)
		# é€šè¿‡å…¨è¿æ¥å±‚
		x = self.fc(x)
		return x
def resnet18():
	# é€šè¿‡è°ƒæ•´æ¨¡å—å†…éƒ¨BasicBlock çš„æ•°é‡å’Œé…ç½®å®ç°ä¸åŒçš„ResNet
	return ResNet([2, 2, 2, 2])
def resnet34():
	# é€šè¿‡è°ƒæ•´æ¨¡å—å†…éƒ¨BasicBlock çš„æ•°é‡å’Œé…ç½®å®ç°ä¸åŒçš„ResNet
	return ResNet([3, 4, 6, 3])
```

## 10 æµ‹é‡å·¥å…·

Keras çš„æµ‹é‡å·¥å…·çš„ä½¿ç”¨æ–¹æ³•ä¸€èˆ¬æœ‰4 ä¸ªä¸»è¦æ­¥éª¤ï¼šæ–°å»ºæµ‹é‡å™¨ï¼Œå†™å…¥æ•°æ®ï¼Œè¯»å–ç»Ÿè®¡æ•°æ®å’Œæ¸…é›¶æµ‹é‡å™¨ã€‚

åœ¨ keras.metrics æ¨¡å—ä¸­ï¼Œæä¾›äº†è¾ƒå¤šçš„å¸¸ç”¨æµ‹é‡å™¨ç±»ï¼Œå¦‚ç»Ÿè®¡å¹³å‡å€¼çš„Mean ç±»ï¼Œç»Ÿè®¡å‡†ç¡®ç‡çš„Accuracy ç±»ï¼Œç»Ÿè®¡ä½™å¼¦ç›¸ä¼¼åº¦çš„CosineSimilarity ç±»ç­‰ã€‚  

```python
acc_meter = metrics.Accuracy() # åˆ›å»ºå‡†ç¡®ç‡æµ‹é‡å™¨
# æ–°å»ºå¹³å‡æµ‹é‡å™¨ï¼Œé€‚åˆLoss æ•°æ®
loss_meter = metrics.Mean()
# è®°å½•é‡‡æ ·çš„æ•°æ®ï¼Œé€šè¿‡float()å‡½æ•°å°†å¼ é‡è½¬æ¢ä¸ºæ™®é€šæ•°å€¼
loss_meter.update_state(float(loss))
# æ‰“å°ç»Ÿè®¡æœŸé—´çš„å¹³å‡loss
print(step, 'loss:', loss_meter.result())
# æµ‹é‡å™¨ä¼šç»Ÿè®¡æ‰€æœ‰å†å²è®°å½•çš„æ•°æ®ï¼Œå› æ­¤åœ¨å¯åŠ¨æ–°ä¸€è½®ç»Ÿè®¡æ—¶ï¼Œæœ‰å¿…è¦æ¸…é™¤å†å²çŠ¶æ€,
if step % 100 == 0:
	# æ‰“å°ç»Ÿè®¡çš„å¹³å‡loss
	print(step, 'loss:', loss_meter.result())
	loss_meter.reset_states() # æ‰“å°å®Œåï¼Œæ¸…é›¶æµ‹é‡å™¨
```








